{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "904d21ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "f826940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnitaryModel(nn.Module):\n",
    "    def __init__(self, num_qubits):\n",
    "        super(UnitaryModel, self).__init__()\n",
    "        d = 2**num_qubits\n",
    "        self.U = orthogonal(nn.Linear(d,d, bias=False, dtype=torch.cfloat), orthogonal_map='matrix_exp')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.U(x)\n",
    "        return x\n",
    "    \n",
    "model = UnitaryModel(num_qubits=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "60ef3e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.+0.j, 0.+0.j],\n",
      "        [1.+0.j, 0.+0.j],\n",
      "        [1.+0.j, 0.+0.j],\n",
      "        [1.+0.j, 0.+0.j]], requires_grad=True)\n",
      "tensor([[1.+0.j, 0.+0.j],\n",
      "        [1.+0.j, 0.+0.j],\n",
      "        [1.+0.j, 0.+0.j],\n",
      "        [1.+0.j, 0.+0.j]])\n"
     ]
    }
   ],
   "source": [
    "#dummy_x = torch.tensor([[1,0],[0,1]], dtype=torch.cfloat, requires_grad=True)\n",
    "#dummy_y = torch.tensor([[1,-1j],[1,1j]],dtype=torch.cfloat)/torch.sqrt(torch.tensor(2))\n",
    "dummy_x = torch.tensor([[1,0],[1,0],[1,0],[1,0]], dtype=torch.cfloat, requires_grad=True)\n",
    "dummy_y = torch.tensor([[1,0],[1,0],[1,0],[1,0]], dtype=torch.cfloat)\n",
    "\n",
    "\n",
    "print(dummy_x)\n",
    "print(dummy_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "e291f983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3331, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def fidelity_loss(output, target, model, reg=1e-6):\n",
    "    loss = 0\n",
    "    for i in range(len(output)):\n",
    "        inner_prod = torch.dot(output[i].T.conj(), target[i])\n",
    "        #loss += inner_prod.conj() * inner_prod\n",
    "        loss -= torch.abs(inner_prod)**2 / len(output)\n",
    "    \n",
    "    U = list(model.parameters())[0].detach()\n",
    "    orth_constraint = torch.matmul(U,U.T.conj()) - torch.eye(U.shape[0])\n",
    "    return loss + reg*orth_constraint.abs().sum()\n",
    "\n",
    "loss = fidelity_loss(dummy_x, dummy_y, model, 1)\n",
    "print(loss)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "5d783d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1000\n",
    "fidelity = torch.zeros(NUM_EPOCHS, dtype=torch.cfloat)\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(dummy_x)\n",
    "    loss = fidelity_loss(output, dummy_y, model, reg=1)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    fidelity[i] = loss.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "2492e5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331, 1.3331,\n",
       "        1.3331])"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fidelity.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "70dcbc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('U.parametrizations.weight.original', Parameter containing:\n",
      "tensor([[-1.0000-0.0030j,  0.0000+0.0000j],\n",
      "        [ 0.8103+0.1589j, -1.0000-0.0021j]], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for layer in model.named_parameters():\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "e888a179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unitarity:  tensor(1.3521)\n",
      "tensor([[-1.0000-0.0030j,  0.0000+0.0000j],\n",
      "        [ 0.8103+0.1589j, -1.0000-0.0021j]])\n",
      "tensor(-1., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "U = model.state_dict()['U.parametrizations.weight.original']\n",
    "print(\"Unitarity: \", torch.dist(U.T.conj() @ U, torch.eye(2)))\n",
    "print(U)\n",
    "\n",
    "print(fidelity_loss(model(dummy_x), dummy_y, model, reg=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "9013fdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.3442e-01-6.7870e-01j, 8.4132e-05-6.6310e-05j],\n",
      "        [7.3442e-01-6.7870e-01j, 8.4132e-05-6.6310e-05j],\n",
      "        [7.3442e-01-6.7870e-01j, 8.4132e-05-6.6310e-05j],\n",
      "        [7.3442e-01-6.7870e-01j, 8.4132e-05-6.6310e-05j]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "tensor([[1.+0.j, 0.+0.j],\n",
      "        [1.+0.j, 0.+0.j],\n",
      "        [1.+0.j, 0.+0.j],\n",
      "        [1.+0.j, 0.+0.j]])\n"
     ]
    }
   ],
   "source": [
    "print(model(dummy_x))\n",
    "print(dummy_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QML",
   "language": "python",
   "name": "qml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
