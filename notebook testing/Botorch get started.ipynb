{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "da6ae7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5]) torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from botorch.models import SingleTaskGP, FixedNoiseGP\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.acquisition import UpperConfidenceBound, qExpectedImprovement\n",
    "from botorch.optim import optimize_acqf\n",
    "\n",
    "\n",
    "train_X = torch.rand(10, 5)\n",
    "Y = 1 - (train_X - 0.5).norm(dim=-1, keepdim=True)  # explicit output dimension\n",
    "Y += 0.1 * torch.rand_like(Y)\n",
    "train_Y = (Y - Y.mean()) / Y.std()\n",
    "\n",
    "print(train_X.shape, train_Y.shape)\n",
    "\n",
    "train_Y_var = torch.full_like(train_Y, 0.2)\n",
    "gp = FixedNoiseGP(train_X, train_Y, train_Y_var)\n",
    "mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
    "fit_gpytorch_model(mll);\n",
    "\n",
    "bounds = torch.stack([torch.zeros(5), torch.ones(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "395f2fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "tensor(1.3541)\n"
     ]
    }
   ],
   "source": [
    "UCB = UpperConfidenceBound(gp, beta=0.1)\n",
    "candidate, acq_value = optimize_acqf(\n",
    "    UCB, bounds=bounds, q=1, num_restarts=15, raw_samples=256\n",
    ")\n",
    "print(candidate.shape)\n",
    "print(acq_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "82232e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n",
      "tensor([1.0025, 1.1652, 1.2731])\n",
      "torch.Size([3, 5])\n",
      "tensor(1.3035)\n"
     ]
    }
   ],
   "source": [
    "qEI = qExpectedImprovement(gp, best_f=0.2)\n",
    "candidate, acq_value = optimize_acqf(\n",
    "    qEI, bounds=bounds, q=3, num_restarts=15, raw_samples=256, sequential=True\n",
    ")\n",
    "print(candidate.shape)\n",
    "print(acq_value)\n",
    "\n",
    "candidate, acq_value = optimize_acqf(\n",
    "    qEI, bounds=bounds, q=3, num_restarts=15, raw_samples=256, sequential=False\n",
    ")\n",
    "print(candidate.shape)\n",
    "print(acq_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "692af010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0221], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "qEI = qExpectedImprovement(gp, best_f=train_Y.max())\n",
    "qei = qEI(train_X)\n",
    "print(qei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab945478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6f3c33a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch.kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c612937f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package gpytorch.kernels in gpytorch:\n",
      "\n",
      "NAME\n",
      "    gpytorch.kernels\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    additive_structure_kernel\n",
      "    arc_kernel\n",
      "    cosine_kernel\n",
      "    cylindrical_kernel\n",
      "    distributional_input_kernel\n",
      "    gaussian_symmetrized_kl_kernel\n",
      "    grid_interpolation_kernel\n",
      "    grid_kernel\n",
      "    index_kernel\n",
      "    inducing_point_kernel\n",
      "    keops (package)\n",
      "    kernel\n",
      "    lcm_kernel\n",
      "    linear_kernel\n",
      "    matern_kernel\n",
      "    multi_device_kernel\n",
      "    multitask_kernel\n",
      "    newton_girard_additive_kernel\n",
      "    periodic_kernel\n",
      "    piecewise_polynomial_kernel\n",
      "    polynomial_kernel\n",
      "    polynomial_kernel_grad\n",
      "    product_structure_kernel\n",
      "    rbf_kernel\n",
      "    rbf_kernel_grad\n",
      "    rff_kernel\n",
      "    rq_kernel\n",
      "    scale_kernel\n",
      "    spectral_delta_kernel\n",
      "    spectral_mixture_kernel\n",
      "\n",
      "CLASSES\n",
      "    gpytorch.module.Module(torch.nn.modules.module.Module)\n",
      "        gpytorch.kernels.kernel.Kernel\n",
      "            gpytorch.kernels.additive_structure_kernel.AdditiveStructureKernel\n",
      "            gpytorch.kernels.arc_kernel.ArcKernel\n",
      "            gpytorch.kernels.cosine_kernel.CosineKernel\n",
      "            gpytorch.kernels.cylindrical_kernel.CylindricalKernel\n",
      "            gpytorch.kernels.distributional_input_kernel.DistributionalInputKernel\n",
      "                gpytorch.kernels.gaussian_symmetrized_kl_kernel.GaussianSymmetrizedKLKernel\n",
      "            gpytorch.kernels.grid_kernel.GridKernel\n",
      "                gpytorch.kernels.grid_interpolation_kernel.GridInterpolationKernel\n",
      "            gpytorch.kernels.index_kernel.IndexKernel\n",
      "            gpytorch.kernels.inducing_point_kernel.InducingPointKernel\n",
      "            gpytorch.kernels.kernel.AdditiveKernel\n",
      "            gpytorch.kernels.kernel.ProductKernel\n",
      "            gpytorch.kernels.lcm_kernel.LCMKernel\n",
      "            gpytorch.kernels.linear_kernel.LinearKernel\n",
      "            gpytorch.kernels.matern_kernel.MaternKernel\n",
      "            gpytorch.kernels.multi_device_kernel.MultiDeviceKernel(torch.nn.parallel.data_parallel.DataParallel, gpytorch.kernels.kernel.Kernel)\n",
      "            gpytorch.kernels.multitask_kernel.MultitaskKernel\n",
      "            gpytorch.kernels.newton_girard_additive_kernel.NewtonGirardAdditiveKernel\n",
      "            gpytorch.kernels.periodic_kernel.PeriodicKernel\n",
      "            gpytorch.kernels.piecewise_polynomial_kernel.PiecewisePolynomialKernel\n",
      "            gpytorch.kernels.polynomial_kernel.PolynomialKernel\n",
      "                gpytorch.kernels.polynomial_kernel_grad.PolynomialKernelGrad\n",
      "            gpytorch.kernels.product_structure_kernel.ProductStructureKernel\n",
      "            gpytorch.kernels.rbf_kernel.RBFKernel\n",
      "                gpytorch.kernels.rbf_kernel_grad.RBFKernelGrad\n",
      "            gpytorch.kernels.rff_kernel.RFFKernel\n",
      "            gpytorch.kernels.rq_kernel.RQKernel\n",
      "            gpytorch.kernels.scale_kernel.ScaleKernel\n",
      "            gpytorch.kernels.spectral_delta_kernel.SpectralDeltaKernel\n",
      "            gpytorch.kernels.spectral_mixture_kernel.SpectralMixtureKernel\n",
      "    torch.nn.parallel.data_parallel.DataParallel(torch.nn.modules.module.Module)\n",
      "        gpytorch.kernels.multi_device_kernel.MultiDeviceKernel(torch.nn.parallel.data_parallel.DataParallel, gpytorch.kernels.kernel.Kernel)\n",
      "    \n",
      "    class AdditiveKernel(Kernel)\n",
      "     |  AdditiveKernel(*kernels)\n",
      "     |  \n",
      "     |  A Kernel that supports summing over multiple component kernels.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> covar_module = RBFKernel(active_dims=torch.tensor([1])) + RBFKernel(active_dims=torch.tensor([2]))\n",
      "     |      >>> x1 = torch.randn(50, 2)\n",
      "     |      >>> additive_kernel_matrix = covar_module(x1)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AdditiveKernel\n",
      "     |      Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __init__(self, *kernels)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, x1, x2, diag=False, **params)\n",
      "     |      Computes the covariance between x1 and x2.\n",
      "     |      This method should be imlemented by all Kernel subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b x n x d`):\n",
      "     |              First set of data\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b x m x d`):\n",
      "     |              Second set of data\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should the Kernel compute the whole kernel, or just the diag?\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              If this is true, it treats the last dimension of the data as another batch dimension.\n",
      "     |              (Useful for additive structure over the dimensions). Default: False\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n",
      "     |              The exact size depends on the kernel's evaluation mode:\n",
      "     |      \n",
      "     |              * `full_covar`: `n x m` or `b x n x m`\n",
      "     |              * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n",
      "     |              * `diag`: `n` or `b x n`\n",
      "     |              * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1, x2)\n",
      "     |      How many outputs are produced per input (default 1)\n",
      "     |      if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel\n",
      "     |      will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`\n",
      "     |      Default: 1\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  is_stationary\n",
      "     |      Kernel is stationary if all components are stationary.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Kernel:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Kernel:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Kernel:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Kernel:\n",
      "     |  \n",
      "     |  has_lengthscale = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class AdditiveStructureKernel(gpytorch.kernels.kernel.Kernel)\n",
      "     |  AdditiveStructureKernel(base_kernel: gpytorch.kernels.kernel.Kernel, num_dims: int, active_dims: Union[Tuple[int, ...], NoneType] = None)\n",
      "     |  \n",
      "     |  A Kernel decorator for kernels with additive structure. If a kernel decomposes\n",
      "     |  additively, then this module will be much more computationally efficient.\n",
      "     |  \n",
      "     |  A kernel function `k` decomposes additively if it can be written as\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |  \n",
      "     |     \\begin{equation*}\n",
      "     |        k(\\mathbf{x_1}, \\mathbf{x_2}) = k'(x_1^{(1)}, x_2^{(1)}) + \\ldots + k'(x_1^{(d)}, x_2^{(d)})\n",
      "     |     \\end{equation*}\n",
      "     |  \n",
      "     |  for some kernel :math:`k'` that operates on a subset of dimensions.\n",
      "     |  \n",
      "     |  Given a `b x n x d` input, `AdditiveStructureKernel` computes `d` one-dimensional kernels\n",
      "     |  (using the supplied base_kernel), and then adds the component kernels together.\n",
      "     |  Unlike :class:`~gpytorch.kernels.AdditiveKernel`, `AdditiveStructureKernel` computes each\n",
      "     |  of the additive terms in batch, making it very fast.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      :attr:`base_kernel` (Kernel):\n",
      "     |          The kernel to approximate with KISS-GP\n",
      "     |      :attr:`num_dims` (int):\n",
      "     |          The dimension of the input data.\n",
      "     |      :attr:`active_dims` (tuple of ints, optional):\n",
      "     |          Passed down to the `base_kernel`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AdditiveStructureKernel\n",
      "     |      gpytorch.kernels.kernel.Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, base_kernel: gpytorch.kernels.kernel.Kernel, num_dims: int, active_dims: Union[Tuple[int, ...], NoneType] = None)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Computes the covariance between x1 and x2.\n",
      "     |      This method should be imlemented by all Kernel subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b x n x d`):\n",
      "     |              First set of data\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b x m x d`):\n",
      "     |              Second set of data\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should the Kernel compute the whole kernel, or just the diag?\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              If this is true, it treats the last dimension of the data as another batch dimension.\n",
      "     |              (Useful for additive structure over the dimensions). Default: False\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n",
      "     |              The exact size depends on the kernel's evaluation mode:\n",
      "     |      \n",
      "     |              * `full_covar`: `n x m` or `b x n x m`\n",
      "     |              * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n",
      "     |              * `diag`: `n` or `b x n`\n",
      "     |              * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1, x2)\n",
      "     |      How many outputs are produced per input (default 1)\n",
      "     |      if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel\n",
      "     |      will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`\n",
      "     |      Default: 1\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  is_stationary\n",
      "     |      Kernel is stationary if the base kernel is stationary.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  has_lengthscale = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class ArcKernel(gpytorch.kernels.kernel.Kernel)\n",
      "     |  ArcKernel(base_kernel: gpytorch.kernels.kernel.Kernel, delta_func: Union[Callable, NoneType] = None, angle_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, radius_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, **kwargs)\n",
      "     |  \n",
      "     |  Computes a covariance matrix based on the Arc Kernel\n",
      "     |  (https://arxiv.org/abs/1409.4011) between inputs :math:`\\mathbf{x_1}`\n",
      "     |  and :math:`\\mathbf{x_2}`. First it applies a cylindrical embedding:\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |      g_{i}(\\mathbf{x}) = \\begin{cases}\n",
      "     |      [0, 0]^{T} & \\delta_{i}(\\mathbf{x}) = \\text{false}\\\\\n",
      "     |      \\omega_{i} \\left[ \\sin{\\pi\\rho_{i}\\frac{x_{i}}{u_{i}-l_{i}}},\n",
      "     |      \\cos{\\pi\\rho_{i}\\frac{x_{i}}{u_{i}-l_{i}}} \\right] & \\text{otherwise}\n",
      "     |      \\end{cases}\n",
      "     |  \n",
      "     |  where\n",
      "     |  * :math:`\\rho` is the angle parameter.\n",
      "     |  * :math:`\\omega` is a radius parameter.\n",
      "     |  \n",
      "     |  then the kernel is built with the particular covariance function, e.g.\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |      \\begin{equation}\n",
      "     |      k_{i}(\\mathbf{x}, \\mathbf{x'}) =\n",
      "     |      \\sigma^{2}\\exp \\left(-\\frac{1}{2}d_{i}(\\mathbf{x}, \\mathbf{x^{'}}) \\right)^{2}\n",
      "     |      \\end{equation}\n",
      "     |  \n",
      "     |  and the produt between dimensions\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |      \\begin{equation}\n",
      "     |      k_{i}(\\mathbf{x}, \\mathbf{x'}) =\n",
      "     |      \\sigma^{2}\\exp \\left(-\\frac{1}{2}d_{i}(\\mathbf{x}, \\mathbf{x^{'}}) \\right)^{2}\n",
      "     |      \\end{equation}\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |      This kernel does not have an `outputscale` parameter. To add a scaling\n",
      "     |      parameter, decorate this kernel with a\n",
      "     |      :class:`gpytorch.kernels.ScaleKernel`.\n",
      "     |      When using with an input of `b x n x d` dimensions, decorate this\n",
      "     |      kernel with :class:`gpytorch.kernel.ProductStructuredKernel , setting\n",
      "     |      the number of dims, `num_dims to d.`\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |      This kernel does not have an ARD lengthscale option.\n",
      "     |  \n",
      "     |  :param base_kernel: (Default :obj:`gpytorch.kernels.MaternKernel(nu=2.5)`.)\n",
      "     |      The euclidean covariance of choice.\n",
      "     |  :type base_kernel: :obj:`~gpytorch.kernels.Kernel`\n",
      "     |  :param ard_num_dims: (Default `None`.) The number of dimensions to compute the kernel for.\n",
      "     |      The kernel has two parameters which are individually defined for each\n",
      "     |      dimension, defaults to None\n",
      "     |  :type ard_num_dims: int, optional\n",
      "     |  :param angle_prior: Set this if you want to apply a prior to the period angle parameter.\n",
      "     |  :type angle_prior: :obj:`~gpytorch.priors.Prior`, optional\n",
      "     |  :param radius_prior: Set this if you want to apply a prior to the lengthscale parameter.\n",
      "     |  :type radius_prior: :obj:`~gpytorch.priors.Prior`, optional\n",
      "     |  \n",
      "     |  :var torch.Tensor radius: The radius parameter. Size = `*batch_shape  x 1`.\n",
      "     |  :var torch.Tensor angle: The period angle parameter. Size = `*batch_shape  x 1`.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> x = torch.randn(10, 5)\n",
      "     |      >>> # Non-batch: Simple option\n",
      "     |      ... base_kernel = gpytorch.kernels.MaternKernel(nu=2.5)\n",
      "     |      >>> base_kernel.raw_lengthscale.requires_grad_(False)\n",
      "     |      >>> covar_module = gpytorch.kernels.ProductStructureKernel(\n",
      "     |              gpytorch.kernels.ScaleKernel(\n",
      "     |                  ArcKernel(base_kernel,\n",
      "     |                            angle_prior=gpytorch.priors.GammaPrior(0.5,1),\n",
      "     |                            radius_prior=gpytorch.priors.GammaPrior(3,2),\n",
      "     |                            ard_num_dims=x.shape[-1])),\n",
      "     |              num_dims=x.shape[-1])\n",
      "     |      >>> covar = covar_module(x)\n",
      "     |      >>> print(covar.shape)\n",
      "     |      >>> # Now with batch\n",
      "     |      >>> covar_module = gpytorch.kernels.ProductStructureKernel(\n",
      "     |              gpytorch.kernels.ScaleKernel(\n",
      "     |                  ArcKernel(base_kernel,\n",
      "     |                            angle_prior=gpytorch.priors.GammaPrior(0.5,1),\n",
      "     |                            radius_prior=gpytorch.priors.GammaPrior(3,2),\n",
      "     |                            ard_num_dims=x.shape[-1])),\n",
      "     |              num_dims=x.shape[-1])\n",
      "     |      >>> covar = covar_module(x\n",
      "     |      >>> print(covar.shape)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ArcKernel\n",
      "     |      gpytorch.kernels.kernel.Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, base_kernel: gpytorch.kernels.kernel.Kernel, delta_func: Union[Callable, NoneType] = None, angle_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, radius_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  default_delta_func(self, x)\n",
      "     |  \n",
      "     |  embedding(self, x)\n",
      "     |  \n",
      "     |  forward(self, x1, x2, diag=False, **params)\n",
      "     |      Computes the covariance between x1 and x2.\n",
      "     |      This method should be imlemented by all Kernel subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b x n x d`):\n",
      "     |              First set of data\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b x m x d`):\n",
      "     |              Second set of data\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should the Kernel compute the whole kernel, or just the diag?\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              If this is true, it treats the last dimension of the data as another batch dimension.\n",
      "     |              (Useful for additive structure over the dimensions). Default: False\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n",
      "     |              The exact size depends on the kernel's evaluation mode:\n",
      "     |      \n",
      "     |              * `full_covar`: `n x m` or `b x n x m`\n",
      "     |              * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n",
      "     |              * `diag`: `n` or `b x n`\n",
      "     |              * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  angle\n",
      "     |  \n",
      "     |  radius\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  has_lengthscale = True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1, x2)\n",
      "     |      How many outputs are produced per input (default 1)\n",
      "     |      if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel\n",
      "     |      will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`\n",
      "     |      Default: 1\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  is_stationary\n",
      "     |      Property to indicate whether kernel is stationary or not.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class CosineKernel(gpytorch.kernels.kernel.Kernel)\n",
      "     |  CosineKernel(period_length_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, period_length_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, **kwargs)\n",
      "     |  \n",
      "     |  Computes a covariance matrix based on the cosine kernel\n",
      "     |  between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`:\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |  \n",
      "     |     \\begin{equation*}\n",
      "     |        k_{\\text{Cosine}}(\\mathbf{x_1}, \\mathbf{x_2}) = \\cos \\left(\n",
      "     |          \\pi \\Vert \\mathbf{x_1} - \\mathbf{x_2} \\Vert_2 / p \\right)\n",
      "     |     \\end{equation*}\n",
      "     |  \n",
      "     |  where :math:`p` is the period length parameter.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      :attr:`batch_shape` (torch.Size, optional):\n",
      "     |          Set this if you want a separate lengthscale for each\n",
      "     |          batch of input data. It should be `b` if :attr:`x1` is a `b x n x d` tensor. Default: `torch.Size([])`\n",
      "     |      :attr:`active_dims` (tuple of ints, optional):\n",
      "     |          Set this if you want to compute the covariance of only a few input dimensions. The ints\n",
      "     |          corresponds to the indices of the dimensions. Default: `None`.\n",
      "     |      :attr:`period_length_prior` (Prior, optional):\n",
      "     |          Set this if you want to apply a prior to the period length parameter.  Default: `None`\n",
      "     |      :attr:`period_length_constraint` (Constraint, optional):\n",
      "     |          Set this if you want to apply a constraint to the period length parameter. Default: `Positive`.\n",
      "     |      :attr:`eps` (float):\n",
      "     |          The minimum value that the lengthscale/period length can take\n",
      "     |          (prevents divide by zero errors). Default: `1e-6`.\n",
      "     |  \n",
      "     |  Attributes:\n",
      "     |      :attr:`period_length` (Tensor):\n",
      "     |          The period length parameter. Size = `*batch_shape x 1 x 1`.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> x = torch.randn(10, 5)\n",
      "     |      >>> # Non-batch: Simple option\n",
      "     |      >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.CosineKernel())\n",
      "     |      >>>\n",
      "     |      >>> batch_x = torch.randn(2, 10, 5)\n",
      "     |      >>> # Batch: Simple option\n",
      "     |      >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.CosineKernel())\n",
      "     |      >>> # Batch: different lengthscale for each batch\n",
      "     |      >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.CosineKernel(batch_shape=torch.Size([2])))\n",
      "     |      >>> covar = covar_module(x)  # Output: LazyVariable of size (2 x 10 x 10)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CosineKernel\n",
      "     |      gpytorch.kernels.kernel.Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, period_length_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, period_length_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, x1, x2, **params)\n",
      "     |      Computes the covariance between x1 and x2.\n",
      "     |      This method should be imlemented by all Kernel subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b x n x d`):\n",
      "     |              First set of data\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b x m x d`):\n",
      "     |              Second set of data\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should the Kernel compute the whole kernel, or just the diag?\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              If this is true, it treats the last dimension of the data as another batch dimension.\n",
      "     |              (Useful for additive structure over the dimensions). Default: False\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n",
      "     |              The exact size depends on the kernel's evaluation mode:\n",
      "     |      \n",
      "     |              * `full_covar`: `n x m` or `b x n x m`\n",
      "     |              * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n",
      "     |              * `diag`: `n` or `b x n`\n",
      "     |              * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  period_length\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  is_stationary = True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1, x2)\n",
      "     |      How many outputs are produced per input (default 1)\n",
      "     |      if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel\n",
      "     |      will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`\n",
      "     |      Default: 1\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  has_lengthscale = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class CylindricalKernel(gpytorch.kernels.kernel.Kernel)\n",
      "     |  CylindricalKernel(num_angular_weights: int, radial_base_kernel: gpytorch.kernels.kernel.Kernel, eps: Union[float, NoneType] = 1e-06, angular_weights_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, angular_weights_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, alpha_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, alpha_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, beta_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, beta_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, **kwargs)\n",
      "     |  \n",
      "     |  Computes a covariance matrix based on the Cylindrical Kernel between\n",
      "     |  inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`.\n",
      "     |  It was proposed in `BOCK: Bayesian Optimization with Cylindrical Kernels`.\n",
      "     |  See http://proceedings.mlr.press/v80/oh18a.html for more details\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |      The data must lie completely within the unit ball.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      :attr:`num_angular_weights` (int):\n",
      "     |          The number of components in the angular kernel\n",
      "     |      :attr:`radial_base_kernel` (gpytorch.kernel):\n",
      "     |          The base kernel for computing the radial kernel\n",
      "     |      :attr:`batch_size` (int, optional):\n",
      "     |          Set this if the data is batch of input data.\n",
      "     |          It should be `b` if :attr:`x1` is a `b x n x d` tensor. Default: `1`\n",
      "     |      :attr:`eps` (float):\n",
      "     |          Small floating point number used to improve numerical stability\n",
      "     |          in kernel computations. Default: `1e-6`\n",
      "     |      :attr:`param_transform` (function, optional):\n",
      "     |          Set this if you want to use something other than softplus to ensure positiveness of parameters.\n",
      "     |      :attr:`inv_param_transform` (function, optional):\n",
      "     |          Set this to allow setting parameters directly in transformed space and sampling from priors.\n",
      "     |          Automatically inferred for common transformations such as torch.exp or torch.nn.functional.softplus.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CylindricalKernel\n",
      "     |      gpytorch.kernels.kernel.Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, num_angular_weights: int, radial_base_kernel: gpytorch.kernels.kernel.Kernel, eps: Union[float, NoneType] = 1e-06, angular_weights_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, angular_weights_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, alpha_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, alpha_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, beta_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, beta_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, x1: torch.Tensor, x2: torch.Tensor, diag: Union[bool, NoneType] = False, **params) -> torch.Tensor\n",
      "     |      Computes the covariance between x1 and x2.\n",
      "     |      This method should be imlemented by all Kernel subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b x n x d`):\n",
      "     |              First set of data\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b x m x d`):\n",
      "     |              Second set of data\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should the Kernel compute the whole kernel, or just the diag?\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              If this is true, it treats the last dimension of the data as another batch dimension.\n",
      "     |              (Useful for additive structure over the dimensions). Default: False\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n",
      "     |              The exact size depends on the kernel's evaluation mode:\n",
      "     |      \n",
      "     |              * `full_covar`: `n x m` or `b x n x m`\n",
      "     |              * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n",
      "     |              * `diag`: `n` or `b x n`\n",
      "     |              * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n",
      "     |  \n",
      "     |  kuma(self, x: torch.Tensor) -> torch.Tensor\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1: torch.Tensor, x2: torch.Tensor) -> int\n",
      "     |      How many outputs are produced per input (default 1)\n",
      "     |      if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel\n",
      "     |      will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`\n",
      "     |      Default: 1\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  alpha\n",
      "     |  \n",
      "     |  angular_weights\n",
      "     |  \n",
      "     |  beta\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  is_stationary\n",
      "     |      Property to indicate whether kernel is stationary or not.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  has_lengthscale = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class DistributionalInputKernel(gpytorch.kernels.kernel.Kernel)\n",
      "     |  DistributionalInputKernel(distance_function: Callable, **kwargs)\n",
      "     |  \n",
      "     |  Computes a covariance matrix over __Gaussian__ distributions via exponentiating the\n",
      "     |  distance function between probability distributions.\n",
      "     |  .. math::\n",
      "     |  \n",
      "     |      \\begin{equation*}\n",
      "     |          k(p(x), p(x')) = \\exp\\{-a d(p(x), p(x'))\\})\n",
      "     |      \\end{equation*}\n",
      "     |  \n",
      "     |  where :math:`a` is the lengthscale.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      :attr:`distance_function` (function) distance function between distributional inputs.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DistributionalInputKernel\n",
      "     |      gpytorch.kernels.kernel.Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, distance_function: Callable, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, x1, x2, diag=False, *args, **kwargs)\n",
      "     |      Computes the covariance between x1 and x2.\n",
      "     |      This method should be imlemented by all Kernel subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b x n x d`):\n",
      "     |              First set of data\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b x m x d`):\n",
      "     |              Second set of data\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should the Kernel compute the whole kernel, or just the diag?\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              If this is true, it treats the last dimension of the data as another batch dimension.\n",
      "     |              (Useful for additive structure over the dimensions). Default: False\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n",
      "     |              The exact size depends on the kernel's evaluation mode:\n",
      "     |      \n",
      "     |              * `full_covar`: `n x m` or `b x n x m`\n",
      "     |              * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n",
      "     |              * `diag`: `n` or `b x n`\n",
      "     |              * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  has_lengthscale = True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1, x2)\n",
      "     |      How many outputs are produced per input (default 1)\n",
      "     |      if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel\n",
      "     |      will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`\n",
      "     |      Default: 1\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  is_stationary\n",
      "     |      Property to indicate whether kernel is stationary or not.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class GaussianSymmetrizedKLKernel(gpytorch.kernels.distributional_input_kernel.DistributionalInputKernel)\n",
      "     |  GaussianSymmetrizedKLKernel(**kwargs)\n",
      "     |  \n",
      "     |  Computes a kernel based on the symmetrized KL divergence, assuming that two Gaussian\n",
      "     |  distributions are inputted. Inputs are assumed to be `batch x N x 2d` tensors where `d` is the\n",
      "     |  dimension of the distribution. The first `d` dimensions are the mean parameters of the\n",
      "     |  `batch x N` distributions, while the second `d` dimensions are the log variances.\n",
      "     |  \n",
      "     |  Original citation is Moreno et al, '04\n",
      "     |  (https://papers.nips.cc/paper/2351-a-kullback-leibler-divergence-based-kernel-for-svm-\\\n",
      "     |  classification-in-multimedia-applications.pdf) for the symmetrized KL divergence kernel between\n",
      "     |  two Gaussian distributions.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GaussianSymmetrizedKLKernel\n",
      "     |      gpytorch.kernels.distributional_input_kernel.DistributionalInputKernel\n",
      "     |      gpytorch.kernels.kernel.Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.distributional_input_kernel.DistributionalInputKernel:\n",
      "     |  \n",
      "     |  forward(self, x1, x2, diag=False, *args, **kwargs)\n",
      "     |      Computes the covariance between x1 and x2.\n",
      "     |      This method should be imlemented by all Kernel subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b x n x d`):\n",
      "     |              First set of data\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b x m x d`):\n",
      "     |              Second set of data\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should the Kernel compute the whole kernel, or just the diag?\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              If this is true, it treats the last dimension of the data as another batch dimension.\n",
      "     |              (Useful for additive structure over the dimensions). Default: False\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n",
      "     |              The exact size depends on the kernel's evaluation mode:\n",
      "     |      \n",
      "     |              * `full_covar`: `n x m` or `b x n x m`\n",
      "     |              * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n",
      "     |              * `diag`: `n` or `b x n`\n",
      "     |              * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from gpytorch.kernels.distributional_input_kernel.DistributionalInputKernel:\n",
      "     |  \n",
      "     |  has_lengthscale = True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1, x2)\n",
      "     |      How many outputs are produced per input (default 1)\n",
      "     |      if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel\n",
      "     |      will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`\n",
      "     |      Default: 1\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  is_stationary\n",
      "     |      Property to indicate whether kernel is stationary or not.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class GridInterpolationKernel(gpytorch.kernels.grid_kernel.GridKernel)\n",
      "     |  GridInterpolationKernel(base_kernel: gpytorch.kernels.kernel.Kernel, grid_size: Union[int, List[int]], num_dims: Union[int, NoneType] = None, grid_bounds: Union[Tuple[float, float], NoneType] = None, active_dims: Union[Tuple[int, ...], NoneType] = None)\n",
      "     |  \n",
      "     |  Implements the KISS-GP (or SKI) approximation for a given kernel.\n",
      "     |  It was proposed in `Kernel Interpolation for Scalable Structured Gaussian Processes`_,\n",
      "     |  and offers extremely fast and accurate Kernel approximations for large datasets.\n",
      "     |  \n",
      "     |  Given a base kernel `k`, the covariance :math:`k(\\mathbf{x_1}, \\mathbf{x_2})` is approximated by\n",
      "     |  using a grid of regularly spaced *inducing points*:\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |  \n",
      "     |     \\begin{equation*}\n",
      "     |        k(\\mathbf{x_1}, \\mathbf{x_2}) = \\mathbf{w_{x_1}}^\\top K_{U,U} \\mathbf{w_{x_2}}\n",
      "     |     \\end{equation*}\n",
      "     |  \n",
      "     |  where\n",
      "     |  \n",
      "     |  * :math:`U` is the set of gridded inducing points\n",
      "     |  \n",
      "     |  * :math:`K_{U,U}` is the kernel matrix between the inducing points\n",
      "     |  \n",
      "     |  * :math:`\\mathbf{w_{x_1}}` and :math:`\\mathbf{w_{x_2}}` are sparse vectors based on\n",
      "     |    :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}` that apply cubic interpolation.\n",
      "     |  \n",
      "     |  The user should supply the size of the grid (using the :attr:`grid_size` attribute).\n",
      "     |  To choose a reasonable grid value, we highly recommend using the\n",
      "     |  :func:`gpytorch.utils.grid.choose_grid_size` helper function.\n",
      "     |  The bounds of the grid will automatically be determined by data.\n",
      "     |  \n",
      "     |  (Alternatively, you can hard-code bounds using the :attr:`grid_bounds`, which\n",
      "     |  will speed up this kernel's computations.)\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |  \n",
      "     |      `GridInterpolationKernel` can only wrap **stationary kernels** (such as RBF, Matern,\n",
      "     |      Periodic, Spectral Mixture, etc.)\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      - :attr:`base_kernel` (Kernel):\n",
      "     |          The kernel to approximate with KISS-GP\n",
      "     |      - :attr:`grid_size` (Union[int, List[int]]):\n",
      "     |          The size of the grid in each dimension.\n",
      "     |          If a single int is provided, then every dimension will have the same grid size.\n",
      "     |      - :attr:`num_dims` (int):\n",
      "     |          The dimension of the input data. Required if `grid_bounds=None`\n",
      "     |      - :attr:`grid_bounds` (tuple(float, float), optional):\n",
      "     |          The bounds of the grid, if known (high performance mode).\n",
      "     |          The length of the tuple must match the number of dimensions.\n",
      "     |          The entries represent the min/max values for each dimension.\n",
      "     |      - :attr:`active_dims` (tuple of ints, optional):\n",
      "     |          Passed down to the `base_kernel`.\n",
      "     |  \n",
      "     |  .. _Kernel Interpolation for Scalable Structured Gaussian Processes:\n",
      "     |      http://proceedings.mlr.press/v37/wilson15.pdf\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GridInterpolationKernel\n",
      "     |      gpytorch.kernels.grid_kernel.GridKernel\n",
      "     |      gpytorch.kernels.kernel.Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, base_kernel: gpytorch.kernels.kernel.Kernel, grid_size: Union[int, List[int]], num_dims: Union[int, NoneType] = None, grid_bounds: Union[Tuple[float, float], NoneType] = None, active_dims: Union[Tuple[int, ...], NoneType] = None)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Computes the covariance between x1 and x2.\n",
      "     |      This method should be imlemented by all Kernel subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b x n x d`):\n",
      "     |              First set of data\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b x m x d`):\n",
      "     |              Second set of data\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should the Kernel compute the whole kernel, or just the diag?\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              If this is true, it treats the last dimension of the data as another batch dimension.\n",
      "     |              (Useful for additive structure over the dimensions). Default: False\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n",
      "     |              The exact size depends on the kernel's evaluation mode:\n",
      "     |      \n",
      "     |              * `full_covar`: `n x m` or `b x n x m`\n",
      "     |              * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n",
      "     |              * `diag`: `n` or `b x n`\n",
      "     |              * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.grid_kernel.GridKernel:\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1, x2)\n",
      "     |      How many outputs are produced per input (default 1)\n",
      "     |      if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel\n",
      "     |      will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`\n",
      "     |      Default: 1\n",
      "     |  \n",
      "     |  register_buffer_list(self, base_name, tensors)\n",
      "     |      Helper to register several buffers at once under a single base name\n",
      "     |  \n",
      "     |  update_grid(self, grid)\n",
      "     |      Supply a new `grid` if it ever changes.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from gpytorch.kernels.grid_kernel.GridKernel:\n",
      "     |  \n",
      "     |  grid\n",
      "     |  \n",
      "     |  is_ragged\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from gpytorch.kernels.grid_kernel.GridKernel:\n",
      "     |  \n",
      "     |  is_stationary = True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  has_lengthscale = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class GridKernel(gpytorch.kernels.kernel.Kernel)\n",
      "     |  GridKernel(base_kernel: gpytorch.kernels.kernel.Kernel, grid: torch.Tensor, interpolation_mode: Union[bool, NoneType] = False, active_dims: Union[bool, NoneType] = None)\n",
      "     |  \n",
      "     |  If the input data :math:`X` are regularly spaced on a grid, then\n",
      "     |  `GridKernel` can dramatically speed up computatations for stationary kernel.\n",
      "     |  \n",
      "     |  GridKernel exploits Toeplitz and Kronecker structure within the covariance matrix.\n",
      "     |  See `Fast kernel learning for multidimensional pattern extrapolation`_ for more info.\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |  \n",
      "     |      `GridKernel` can only wrap **stationary kernels** (such as RBF, Matern,\n",
      "     |      Periodic, Spectral Mixture, etc.)\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      :attr:`base_kernel` (Kernel):\n",
      "     |          The kernel to speed up with grid methods.\n",
      "     |      :attr:`grid` (Tensor):\n",
      "     |          A g x d tensor where column i consists of the projections of the\n",
      "     |          grid in dimension i.\n",
      "     |      :attr:`active_dims` (tuple of ints, optional):\n",
      "     |          Passed down to the `base_kernel`.\n",
      "     |      :attr:`interpolation_mode` (bool):\n",
      "     |          Used for GridInterpolationKernel where we want the covariance\n",
      "     |          between points in the projections of the grid of each dimension.\n",
      "     |          We do this by treating `grid` as d batches of g x 1 tensors by\n",
      "     |          calling base_kernel(grid, grid) with last_dim_is_batch to get a d x g x g Tensor\n",
      "     |          which we Kronecker product to get a g x g KroneckerProductLazyTensor.\n",
      "     |  \n",
      "     |  .. _Fast kernel learning for multidimensional pattern extrapolation:\n",
      "     |      http://www.cs.cmu.edu/~andrewgw/manet.pdf\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GridKernel\n",
      "     |      gpytorch.kernels.kernel.Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, base_kernel: gpytorch.kernels.kernel.Kernel, grid: torch.Tensor, interpolation_mode: Union[bool, NoneType] = False, active_dims: Union[bool, NoneType] = None)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Computes the covariance between x1 and x2.\n",
      "     |      This method should be imlemented by all Kernel subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b x n x d`):\n",
      "     |              First set of data\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b x m x d`):\n",
      "     |              Second set of data\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should the Kernel compute the whole kernel, or just the diag?\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              If this is true, it treats the last dimension of the data as another batch dimension.\n",
      "     |              (Useful for additive structure over the dimensions). Default: False\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n",
      "     |              The exact size depends on the kernel's evaluation mode:\n",
      "     |      \n",
      "     |              * `full_covar`: `n x m` or `b x n x m`\n",
      "     |              * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n",
      "     |              * `diag`: `n` or `b x n`\n",
      "     |              * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1, x2)\n",
      "     |      How many outputs are produced per input (default 1)\n",
      "     |      if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel\n",
      "     |      will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`\n",
      "     |      Default: 1\n",
      "     |  \n",
      "     |  register_buffer_list(self, base_name, tensors)\n",
      "     |      Helper to register several buffers at once under a single base name\n",
      "     |  \n",
      "     |  update_grid(self, grid)\n",
      "     |      Supply a new `grid` if it ever changes.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  grid\n",
      "     |  \n",
      "     |  is_ragged\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  is_stationary = True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  has_lengthscale = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class IndexKernel(gpytorch.kernels.kernel.Kernel)\n",
      "     |  IndexKernel(num_tasks: int, rank: Union[int, NoneType] = 1, prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, var_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, **kwargs)\n",
      "     |  \n",
      "     |  A kernel for discrete indices. Kernel is defined by a lookup table.\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |  \n",
      "     |      \\begin{equation}\n",
      "     |          k(i, j) = \\left(BB^\\top + \\text{diag}(\\mathbf v) \\right)_{i, j}\n",
      "     |      \\end{equation}\n",
      "     |  \n",
      "     |  where :math:`B` is a low-rank matrix, and :math:`\\mathbf v` is a  non-negative vector.\n",
      "     |  These parameters are learned.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      :attr:`num_tasks` (int):\n",
      "     |          Total number of indices.\n",
      "     |      :attr:`batch_shape` (torch.Size, optional):\n",
      "     |          Set if the MultitaskKernel is operating on batches of data (and you want different\n",
      "     |          parameters for each batch)\n",
      "     |      :attr:`rank` (int):\n",
      "     |          Rank of :math:`B` matrix. Controls the degree of\n",
      "     |          correlation between the outputs. With a rank of 1 the\n",
      "     |          outputs are identical except for a scaling factor.\n",
      "     |      :attr:`prior` (:obj:`gpytorch.priors.Prior`):\n",
      "     |          Prior for :math:`B` matrix.\n",
      "     |      :attr:`var_constraint` (Constraint, optional):\n",
      "     |          Constraint for added diagonal component. Default: `Positive`.\n",
      "     |  \n",
      "     |  Attributes:\n",
      "     |      covar_factor:\n",
      "     |          The :math:`B` matrix.\n",
      "     |      raw_var:\n",
      "     |          The element-wise log of the :math:`\\mathbf v` vector.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      IndexKernel\n",
      "     |      gpytorch.kernels.kernel.Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, num_tasks: int, rank: Union[int, NoneType] = 1, prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, var_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, i1, i2, **params)\n",
      "     |      Computes the covariance between x1 and x2.\n",
      "     |      This method should be imlemented by all Kernel subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b x n x d`):\n",
      "     |              First set of data\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b x m x d`):\n",
      "     |              Second set of data\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should the Kernel compute the whole kernel, or just the diag?\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              If this is true, it treats the last dimension of the data as another batch dimension.\n",
      "     |              (Useful for additive structure over the dimensions). Default: False\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n",
      "     |              The exact size depends on the kernel's evaluation mode:\n",
      "     |      \n",
      "     |              * `full_covar`: `n x m` or `b x n x m`\n",
      "     |              * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n",
      "     |              * `diag`: `n` or `b x n`\n",
      "     |              * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  covar_matrix\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  var\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1, x2)\n",
      "     |      How many outputs are produced per input (default 1)\n",
      "     |      if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel\n",
      "     |      will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`\n",
      "     |      Default: 1\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  is_stationary\n",
      "     |      Property to indicate whether kernel is stationary or not.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  has_lengthscale = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class InducingPointKernel(gpytorch.kernels.kernel.Kernel)\n",
      "     |  InducingPointKernel(base_kernel: gpytorch.kernels.kernel.Kernel, inducing_points: torch.Tensor, likelihood: gpytorch.likelihoods.likelihood.Likelihood, active_dims: Union[Tuple[int, ...], NoneType] = None)\n",
      "     |  \n",
      "     |  Kernels in GPyTorch are implemented as a :class:`gpytorch.Module` that, when called on two :obj:`torch.tensor`\n",
      "     |  objects `x1` and `x2` returns either a :obj:`torch.tensor` or a :obj:`gpytorch.lazy.LazyTensor` that represents\n",
      "     |  the covariance matrix between `x1` and `x2`.\n",
      "     |  \n",
      "     |  In the typical use case, to extend this class means to implement the :func:`~gpytorch.kernels.Kernel.forward`\n",
      "     |  method.\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |      The :func:`~gpytorch.kernels.Kernel.__call__` does some additional internal work. In particular,\n",
      "     |      all kernels are lazily evaluated so that, in some cases, we can index in to the kernel matrix before actually\n",
      "     |      computing it. Furthermore, many built in kernel modules return LazyTensors that allow for more efficient\n",
      "     |      inference than if we explicitly computed the kernel matrix itself.\n",
      "     |  \n",
      "     |      As a result, if you want to use a :obj:`gpytorch.kernels.Kernel` object just to get an actual\n",
      "     |      :obj:`torch.tensor` representing the covariance matrix, you may need to call the\n",
      "     |      :func:`gpytorch.lazy.LazyTensor.evaluate` method on the output.\n",
      "     |  \n",
      "     |  This base :class:`Kernel` class includes a lengthscale parameter\n",
      "     |  :math:`\\Theta`, which is used by many common kernel functions.\n",
      "     |  There are a few options for the lengthscale:\n",
      "     |  \n",
      "     |  * Default: No lengthscale (i.e. :math:`\\Theta` is the identity matrix).\n",
      "     |  \n",
      "     |  * Single lengthscale: One lengthscale can be applied to all input dimensions/batches\n",
      "     |    (i.e. :math:`\\Theta` is a constant diagonal matrix).\n",
      "     |    This is controlled by setting the attribute `has_lengthscale=True`.\n",
      "     |  \n",
      "     |  * ARD: Each input dimension gets its own separate lengthscale\n",
      "     |    (i.e. :math:`\\Theta` is a non-constant diagonal matrix).\n",
      "     |    This is controlled by the `ard_num_dims` keyword argument (as well as `has_lengthscale=True`).\n",
      "     |  \n",
      "     |  In batch-mode (i.e. when :math:`x_1` and :math:`x_2` are batches of input matrices), each\n",
      "     |  batch of data can have its own lengthscale parameter by setting the `batch_shape`\n",
      "     |  keyword argument to the appropriate number of batches.\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |  \n",
      "     |      The :attr:`lengthscale` parameter is parameterized on a log scale to constrain it to be positive.\n",
      "     |      You can set a prior on this parameter using the :attr:`lengthscale_prior` argument.\n",
      "     |  \n",
      "     |  Base Args:\n",
      "     |      :attr:`ard_num_dims` (int, optional):\n",
      "     |          Set this if you want a separate lengthscale for each input\n",
      "     |          dimension. It should be `d` if :attr:`x1` is a `n x d` matrix.  Default: `None`\n",
      "     |      :attr:`batch_shape` (torch.Size, optional):\n",
      "     |          Set this if you want a separate lengthscale for each batch of input\n",
      "     |          data. It should be `b1 x ... x bk` if :attr:`x1` is a `b1 x ... x bk x n x d` tensor.\n",
      "     |      :attr:`active_dims` (tuple of ints, optional):\n",
      "     |          Set this if you want to compute the covariance of only a few input dimensions. The ints\n",
      "     |          corresponds to the indices of the dimensions. Default: `None`.\n",
      "     |      :attr:`lengthscale_prior` (Prior, optional):\n",
      "     |          Set this if you want to apply a prior to the lengthscale parameter.  Default: `None`\n",
      "     |      :attr:`lengthscale_constraint` (Constraint, optional):\n",
      "     |          Set this if you want to apply a constraint to the lengthscale parameter. Default: `Positive`.\n",
      "     |      :attr:`eps` (float):\n",
      "     |          The minimum value that the lengthscale can take (prevents divide by zero errors). Default: `1e-6`.\n",
      "     |  \n",
      "     |  Base Attributes:\n",
      "     |      :attr:`lengthscale` (Tensor):\n",
      "     |          The lengthscale parameter. Size/shape of parameter depends on the\n",
      "     |          :attr:`ard_num_dims` and :attr:`batch_shape` arguments.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> covar_module = gpytorch.kernels.LinearKernel()\n",
      "     |      >>> x1 = torch.randn(50, 3)\n",
      "     |      >>> lazy_covar_matrix = covar_module(x1) # Returns a RootLazyTensor\n",
      "     |      >>> tensor_covar_matrix = lazy_covar_matrix.evaluate() # Gets the actual tensor for this kernel matrix\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      InducingPointKernel\n",
      "     |      gpytorch.kernels.kernel.Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __deepcopy__(self, memo)\n",
      "     |  \n",
      "     |  __init__(self, base_kernel: gpytorch.kernels.kernel.Kernel, inducing_points: torch.Tensor, likelihood: gpytorch.likelihoods.likelihood.Likelihood, active_dims: Union[Tuple[int, ...], NoneType] = None)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, x1, x2, diag=False, **kwargs)\n",
      "     |      Computes the covariance between x1 and x2.\n",
      "     |      This method should be imlemented by all Kernel subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b x n x d`):\n",
      "     |              First set of data\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b x m x d`):\n",
      "     |              Second set of data\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should the Kernel compute the whole kernel, or just the diag?\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              If this is true, it treats the last dimension of the data as another batch dimension.\n",
      "     |              (Useful for additive structure over the dimensions). Default: False\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n",
      "     |              The exact size depends on the kernel's evaluation mode:\n",
      "     |      \n",
      "     |              * `full_covar`: `n x m` or `b x n x m`\n",
      "     |              * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n",
      "     |              * `diag`: `n` or `b x n`\n",
      "     |              * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1, x2)\n",
      "     |      How many outputs are produced per input (default 1)\n",
      "     |      if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel\n",
      "     |      will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`\n",
      "     |      Default: 1\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  is_stationary\n",
      "     |      Property to indicate whether kernel is stationary or not.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  has_lengthscale = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class Kernel(gpytorch.module.Module)\n",
      "     |  Kernel(ard_num_dims: Union[int, NoneType] = None, batch_shape: Union[torch.Size, NoneType] = torch.Size([]), active_dims: Union[Tuple[int, ...], NoneType] = None, lengthscale_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, lengthscale_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, eps: Union[float, NoneType] = 1e-06, **kwargs)\n",
      "     |  \n",
      "     |  Kernels in GPyTorch are implemented as a :class:`gpytorch.Module` that, when called on two :obj:`torch.tensor`\n",
      "     |  objects `x1` and `x2` returns either a :obj:`torch.tensor` or a :obj:`gpytorch.lazy.LazyTensor` that represents\n",
      "     |  the covariance matrix between `x1` and `x2`.\n",
      "     |  \n",
      "     |  In the typical use case, to extend this class means to implement the :func:`~gpytorch.kernels.Kernel.forward`\n",
      "     |  method.\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |      The :func:`~gpytorch.kernels.Kernel.__call__` does some additional internal work. In particular,\n",
      "     |      all kernels are lazily evaluated so that, in some cases, we can index in to the kernel matrix before actually\n",
      "     |      computing it. Furthermore, many built in kernel modules return LazyTensors that allow for more efficient\n",
      "     |      inference than if we explicitly computed the kernel matrix itself.\n",
      "     |  \n",
      "     |      As a result, if you want to use a :obj:`gpytorch.kernels.Kernel` object just to get an actual\n",
      "     |      :obj:`torch.tensor` representing the covariance matrix, you may need to call the\n",
      "     |      :func:`gpytorch.lazy.LazyTensor.evaluate` method on the output.\n",
      "     |  \n",
      "     |  This base :class:`Kernel` class includes a lengthscale parameter\n",
      "     |  :math:`\\Theta`, which is used by many common kernel functions.\n",
      "     |  There are a few options for the lengthscale:\n",
      "     |  \n",
      "     |  * Default: No lengthscale (i.e. :math:`\\Theta` is the identity matrix).\n",
      "     |  \n",
      "     |  * Single lengthscale: One lengthscale can be applied to all input dimensions/batches\n",
      "     |    (i.e. :math:`\\Theta` is a constant diagonal matrix).\n",
      "     |    This is controlled by setting the attribute `has_lengthscale=True`.\n",
      "     |  \n",
      "     |  * ARD: Each input dimension gets its own separate lengthscale\n",
      "     |    (i.e. :math:`\\Theta` is a non-constant diagonal matrix).\n",
      "     |    This is controlled by the `ard_num_dims` keyword argument (as well as `has_lengthscale=True`).\n",
      "     |  \n",
      "     |  In batch-mode (i.e. when :math:`x_1` and :math:`x_2` are batches of input matrices), each\n",
      "     |  batch of data can have its own lengthscale parameter by setting the `batch_shape`\n",
      "     |  keyword argument to the appropriate number of batches.\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |  \n",
      "     |      The :attr:`lengthscale` parameter is parameterized on a log scale to constrain it to be positive.\n",
      "     |      You can set a prior on this parameter using the :attr:`lengthscale_prior` argument.\n",
      "     |  \n",
      "     |  Base Args:\n",
      "     |      :attr:`ard_num_dims` (int, optional):\n",
      "     |          Set this if you want a separate lengthscale for each input\n",
      "     |          dimension. It should be `d` if :attr:`x1` is a `n x d` matrix.  Default: `None`\n",
      "     |      :attr:`batch_shape` (torch.Size, optional):\n",
      "     |          Set this if you want a separate lengthscale for each batch of input\n",
      "     |          data. It should be `b1 x ... x bk` if :attr:`x1` is a `b1 x ... x bk x n x d` tensor.\n",
      "     |      :attr:`active_dims` (tuple of ints, optional):\n",
      "     |          Set this if you want to compute the covariance of only a few input dimensions. The ints\n",
      "     |          corresponds to the indices of the dimensions. Default: `None`.\n",
      "     |      :attr:`lengthscale_prior` (Prior, optional):\n",
      "     |          Set this if you want to apply a prior to the lengthscale parameter.  Default: `None`\n",
      "     |      :attr:`lengthscale_constraint` (Constraint, optional):\n",
      "     |          Set this if you want to apply a constraint to the lengthscale parameter. Default: `Positive`.\n",
      "     |      :attr:`eps` (float):\n",
      "     |          The minimum value that the lengthscale can take (prevents divide by zero errors). Default: `1e-6`.\n",
      "     |  \n",
      "     |  Base Attributes:\n",
      "     |      :attr:`lengthscale` (Tensor):\n",
      "     |          The lengthscale parameter. Size/shape of parameter depends on the\n",
      "     |          :attr:`ard_num_dims` and :attr:`batch_shape` arguments.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> covar_module = gpytorch.kernels.LinearKernel()\n",
      "     |      >>> x1 = torch.randn(50, 3)\n",
      "     |      >>> lazy_covar_matrix = covar_module(x1) # Returns a RootLazyTensor\n",
      "     |      >>> tensor_covar_matrix = lazy_covar_matrix.evaluate() # Gets the actual tensor for this kernel matrix\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __init__(self, ard_num_dims: Union[int, NoneType] = None, batch_shape: Union[torch.Size, NoneType] = torch.Size([]), active_dims: Union[Tuple[int, ...], NoneType] = None, lengthscale_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, lengthscale_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, eps: Union[float, NoneType] = 1e-06, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Computes the covariance between x1 and x2.\n",
      "     |      This method should be imlemented by all Kernel subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b x n x d`):\n",
      "     |              First set of data\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b x m x d`):\n",
      "     |              Second set of data\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should the Kernel compute the whole kernel, or just the diag?\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              If this is true, it treats the last dimension of the data as another batch dimension.\n",
      "     |              (Useful for additive structure over the dimensions). Default: False\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n",
      "     |              The exact size depends on the kernel's evaluation mode:\n",
      "     |      \n",
      "     |              * `full_covar`: `n x m` or `b x n x m`\n",
      "     |              * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n",
      "     |              * `diag`: `n` or `b x n`\n",
      "     |              * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1, x2)\n",
      "     |      How many outputs are produced per input (default 1)\n",
      "     |      if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel\n",
      "     |      will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`\n",
      "     |      Default: 1\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  is_stationary\n",
      "     |      Property to indicate whether kernel is stationary or not.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  has_lengthscale = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class LCMKernel(gpytorch.kernels.kernel.Kernel)\n",
      "     |  LCMKernel(base_kernels: List[gpytorch.kernels.kernel.Kernel], num_tasks: int, rank: Union[int, List, NoneType] = 1, task_covar_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None)\n",
      "     |  \n",
      "     |  This kernel supports the LCM kernel. It allows the user to specify a list of\n",
      "     |  base kernels to use, and individual `MultitaskKernel` objects are fit to each\n",
      "     |  of them. The final kernel is the linear sum of the Kronecker product of all\n",
      "     |  these base kernels with their respective `MultitaskKernel` objects.\n",
      "     |  \n",
      "     |  The returned object is of type :obj:`gpytorch.lazy.KroneckerProductLazyTensor`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LCMKernel\n",
      "     |      gpytorch.kernels.kernel.Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __init__(self, base_kernels: List[gpytorch.kernels.kernel.Kernel], num_tasks: int, rank: Union[int, List, NoneType] = 1, task_covar_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None)\n",
      "     |      Args:\n",
      "     |          base_kernels (:type: list of `Kernel` objects): A list of base kernels.\n",
      "     |          num_tasks (int): The number of output tasks to fit.\n",
      "     |          rank (int): Rank of index kernel to use for task covariance matrix for each\n",
      "     |                      of the base kernels.\n",
      "     |          task_covar_prior (:obj:`gpytorch.priors.Prior`): Prior to use for each\n",
      "     |              task kernel. See :class:`gpytorch.kernels.IndexKernel` for details.\n",
      "     |  \n",
      "     |  forward(self, x1, x2, **params)\n",
      "     |      Computes the covariance between x1 and x2.\n",
      "     |      This method should be imlemented by all Kernel subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b x n x d`):\n",
      "     |              First set of data\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b x m x d`):\n",
      "     |              Second set of data\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should the Kernel compute the whole kernel, or just the diag?\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              If this is true, it treats the last dimension of the data as another batch dimension.\n",
      "     |              (Useful for additive structure over the dimensions). Default: False\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n",
      "     |              The exact size depends on the kernel's evaluation mode:\n",
      "     |      \n",
      "     |              * `full_covar`: `n x m` or `b x n x m`\n",
      "     |              * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n",
      "     |              * `diag`: `n` or `b x n`\n",
      "     |              * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1, x2)\n",
      "     |      Given `n` data points `x1` and `m` datapoints `x2`, this multitask kernel\n",
      "     |      returns an `(n*num_tasks) x (m*num_tasks)` covariance matrix.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  is_stationary\n",
      "     |      Property to indicate whether kernel is stationary or not.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  has_lengthscale = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class LinearKernel(gpytorch.kernels.kernel.Kernel)\n",
      "     |  LinearKernel(num_dimensions: Union[int, NoneType] = None, offset_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, variance_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, variance_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, **kwargs)\n",
      "     |  \n",
      "     |  Computes a covariance matrix based on the Linear kernel\n",
      "     |  between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`:\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |      \\begin{equation*}\n",
      "     |          k_\\text{Linear}(\\mathbf{x_1}, \\mathbf{x_2}) = v\\mathbf{x_1}^\\top\n",
      "     |          \\mathbf{x_2}.\n",
      "     |      \\end{equation*}\n",
      "     |  \n",
      "     |  where\n",
      "     |  \n",
      "     |  * :math:`v` is a :attr:`variance` parameter.\n",
      "     |  \n",
      "     |  \n",
      "     |  .. note::\n",
      "     |  \n",
      "     |      To implement this efficiently, we use a :obj:`gpytorch.lazy.RootLazyTensor` during training and a\n",
      "     |      :class:`gpytorch.lazy.MatmulLazyTensor` during test. These lazy tensors represent matrices of the form\n",
      "     |      :math:`K = XX^{\\top}` and :math:`K = XZ^{\\top}`. This makes inference\n",
      "     |      efficient because a matrix-vector product :math:`Kv` can be computed as\n",
      "     |      :math:`Kv=X(X^{\\top}v)`, where the base multiply :math:`Xv` takes only\n",
      "     |      :math:`O(nd)` time and space.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      :attr:`variance_prior` (:class:`gpytorch.priors.Prior`):\n",
      "     |          Prior over the variance parameter (default `None`).\n",
      "     |      :attr:`variance_constraint` (Constraint, optional):\n",
      "     |          Constraint to place on variance parameter. Default: `Positive`.\n",
      "     |      :attr:`active_dims` (list):\n",
      "     |          List of data dimensions to operate on.\n",
      "     |          `len(active_dims)` should equal `num_dimensions`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LinearKernel\n",
      "     |      gpytorch.kernels.kernel.Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, num_dimensions: Union[int, NoneType] = None, offset_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, variance_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, variance_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Computes the covariance between x1 and x2.\n",
      "     |      This method should be imlemented by all Kernel subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b x n x d`):\n",
      "     |              First set of data\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b x m x d`):\n",
      "     |              Second set of data\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should the Kernel compute the whole kernel, or just the diag?\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              If this is true, it treats the last dimension of the data as another batch dimension.\n",
      "     |              (Useful for additive structure over the dimensions). Default: False\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n",
      "     |              The exact size depends on the kernel's evaluation mode:\n",
      "     |      \n",
      "     |              * `full_covar`: `n x m` or `b x n x m`\n",
      "     |              * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n",
      "     |              * `diag`: `n` or `b x n`\n",
      "     |              * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  variance\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1, x2)\n",
      "     |      How many outputs are produced per input (default 1)\n",
      "     |      if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel\n",
      "     |      will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`\n",
      "     |      Default: 1\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  is_stationary\n",
      "     |      Property to indicate whether kernel is stationary or not.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  has_lengthscale = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class MaternKernel(gpytorch.kernels.kernel.Kernel)\n",
      "     |  MaternKernel(nu: Union[float, NoneType] = 2.5, **kwargs)\n",
      "     |  \n",
      "     |  Computes a covariance matrix based on the Matern kernel\n",
      "     |  between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`:\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |  \n",
      "     |     \\begin{equation*}\n",
      "     |        k_{\\text{Matern}}(\\mathbf{x_1}, \\mathbf{x_2}) = \\frac{2^{1 - \\nu}}{\\Gamma(\\nu)}\n",
      "     |        \\left( \\sqrt{2 \\nu} d \\right)^{\\nu} K_\\nu \\left( \\sqrt{2 \\nu} d \\right)\n",
      "     |     \\end{equation*}\n",
      "     |  \n",
      "     |  where\n",
      "     |  \n",
      "     |  * :math:`d = (\\mathbf{x_1} - \\mathbf{x_2})^\\top \\Theta^{-2} (\\mathbf{x_1} - \\mathbf{x_2})`\n",
      "     |    is the distance between\n",
      "     |    :math:`x_1` and :math:`x_2` scaled by the :attr:`lengthscale` parameter :math:`\\Theta`.\n",
      "     |  * :math:`\\nu` is a smoothness parameter (takes values 1/2, 3/2, or 5/2). Smaller values are less smooth.\n",
      "     |  * :math:`K_\\nu` is a modified Bessel function.\n",
      "     |  \n",
      "     |  There are a few options for the lengthscale parameter :math:`\\Theta`:\n",
      "     |  See :class:`gpytorch.kernels.Kernel` for descriptions of the lengthscale options.\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |  \n",
      "     |      This kernel does not have an `outputscale` parameter. To add a scaling parameter,\n",
      "     |      decorate this kernel with a :class:`gpytorch.kernels.ScaleKernel`.\n",
      "     |  \n",
      "     |  :param nu: (Default: 2.5) The smoothness parameter.\n",
      "     |  :type nu: float (0.5, 1.5, or 2.5)\n",
      "     |  :param ard_num_dims: (Default: `None`) Set this if you want a separate lengthscale for each\n",
      "     |      input dimension. It should be `d` if :attr:`x1` is a `... x n x d` matrix.\n",
      "     |  :type ard_num_dims: int, optional\n",
      "     |  :param batch_shape: (Default: `None`) Set this if you want a separate lengthscale for each\n",
      "     |       batch of input data. It should be `torch.Size([b1, b2])` for a `b1 x b2 x n x m` kernel output.\n",
      "     |  :type batch_shape: torch.Size, optional\n",
      "     |  :param active_dims: (Default: `None`) Set this if you want to\n",
      "     |      compute the covariance of only a few input dimensions. The ints\n",
      "     |      corresponds to the indices of the dimensions.\n",
      "     |  :type active_dims: Tuple(int)\n",
      "     |  :param lengthscale_prior: (Default: `None`)\n",
      "     |      Set this if you want to apply a prior to the lengthscale parameter.\n",
      "     |  :type lengthscale_prior: ~gpytorch.priors.Prior, optional\n",
      "     |  :param lengthscale_constraint: (Default: `Positive`) Set this if you want\n",
      "     |      to apply a constraint to the lengthscale parameter.\n",
      "     |  :type lengthscale_constraint: ~gpytorch.constraints.Interval, optional\n",
      "     |  :param eps: (Default: 1e-6) The minimum value that the lengthscale can take (prevents divide by zero errors).\n",
      "     |  :type eps: float, optional\n",
      "     |  \n",
      "     |  :var torch.Tensor lengthscale: The lengthscale parameter. Size/shape of parameter depends on the\n",
      "     |      :attr:`ard_num_dims` and :attr:`batch_shape` arguments.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> x = torch.randn(10, 5)\n",
      "     |      >>> # Non-batch: Simple option\n",
      "     |      >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=0.5))\n",
      "     |      >>> # Non-batch: ARD (different lengthscale for each input dimension)\n",
      "     |      >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=0.5, ard_num_dims=5))\n",
      "     |      >>> covar = covar_module(x)  # Output: LazyVariable of size (10 x 10)\n",
      "     |      >>>\n",
      "     |      >>> batch_x = torch.randn(2, 10, 5)\n",
      "     |      >>> # Batch: Simple option\n",
      "     |      >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=0.5))\n",
      "     |      >>> # Batch: different lengthscale for each batch\n",
      "     |      >>> covar_module = gpytorch.kernels.MaternKernel(nu=0.5, batch_shape=torch.Size([2])\n",
      "     |      >>> covar = covar_module(x)  # Output: LazyVariable of size (2 x 10 x 10)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MaternKernel\n",
      "     |      gpytorch.kernels.kernel.Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, nu: Union[float, NoneType] = 2.5, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, x1, x2, diag=False, **params)\n",
      "     |      Computes the covariance between x1 and x2.\n",
      "     |      This method should be imlemented by all Kernel subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b x n x d`):\n",
      "     |              First set of data\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b x m x d`):\n",
      "     |              Second set of data\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should the Kernel compute the whole kernel, or just the diag?\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              If this is true, it treats the last dimension of the data as another batch dimension.\n",
      "     |              (Useful for additive structure over the dimensions). Default: False\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n",
      "     |              The exact size depends on the kernel's evaluation mode:\n",
      "     |      \n",
      "     |              * `full_covar`: `n x m` or `b x n x m`\n",
      "     |              * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n",
      "     |              * `diag`: `n` or `b x n`\n",
      "     |              * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  has_lengthscale = True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1, x2)\n",
      "     |      How many outputs are produced per input (default 1)\n",
      "     |      if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel\n",
      "     |      will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`\n",
      "     |      Default: 1\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  is_stationary\n",
      "     |      Property to indicate whether kernel is stationary or not.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class MultiDeviceKernel(torch.nn.parallel.data_parallel.DataParallel, gpytorch.kernels.kernel.Kernel)\n",
      "     |  MultiDeviceKernel(base_kernel: gpytorch.kernels.kernel.Kernel, device_ids: List[torch.device], output_device: Union[torch.device, NoneType] = None, create_cuda_context: Union[bool, NoneType] = True, **kwargs)\n",
      "     |  \n",
      "     |  Allocates the covariance matrix on distributed devices, e.g. multiple GPUs.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      - :attr:`base_kernel`: Base kernel to distribute\n",
      "     |      - :attr:`device_ids`: list of `torch.device` objects to place kernel chunks on\n",
      "     |      - :attr:`output_device`: Device where outputs will be placed\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiDeviceKernel\n",
      "     |      torch.nn.parallel.data_parallel.DataParallel\n",
      "     |      gpytorch.kernels.kernel.Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, base_kernel: gpytorch.kernels.kernel.Kernel, device_ids: List[torch.device], output_device: Union[torch.device, NoneType] = None, create_cuda_context: Union[bool, NoneType] = True, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, x1, x2, diag=False, **kwargs)\n",
      "     |      Computes the covariance between x1 and x2.\n",
      "     |      This method should be imlemented by all Kernel subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b x n x d`):\n",
      "     |              First set of data\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b x m x d`):\n",
      "     |              Second set of data\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should the Kernel compute the whole kernel, or just the diag?\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              If this is true, it treats the last dimension of the data as another batch dimension.\n",
      "     |              (Useful for additive structure over the dimensions). Default: False\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n",
      "     |              The exact size depends on the kernel's evaluation mode:\n",
      "     |      \n",
      "     |              * `full_covar`: `n x m` or `b x n x m`\n",
      "     |              * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n",
      "     |              * `diag`: `n` or `b x n`\n",
      "     |              * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n",
      "     |  \n",
      "     |  gather(self, outputs, output_device)\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1, x2)\n",
      "     |      How many outputs are produced per input (default 1)\n",
      "     |      if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel\n",
      "     |      will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`\n",
      "     |      Default: 1\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  base_kernel\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.parallel.data_parallel.DataParallel:\n",
      "     |  \n",
      "     |  parallel_apply(self, replicas, inputs, kwargs)\n",
      "     |  \n",
      "     |  replicate(self, module, device_ids)\n",
      "     |  \n",
      "     |  scatter(self, inputs, kwargs, device_ids)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  is_stationary\n",
      "     |      Property to indicate whether kernel is stationary or not.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  has_lengthscale = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class MultitaskKernel(gpytorch.kernels.kernel.Kernel)\n",
      "     |  MultitaskKernel(data_covar_module: gpytorch.kernels.kernel.Kernel, num_tasks: int, rank: Union[int, NoneType] = 1, task_covar_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, **kwargs)\n",
      "     |  \n",
      "     |  Kernel supporting Kronecker style multitask Gaussian processes (where every data point is evaluated at every\n",
      "     |  task) using :class:`gpytorch.kernels.IndexKernel` as a basic multitask kernel.\n",
      "     |  \n",
      "     |  Given a base covariance module to be used for the data, :math:`K_{XX}`, this kernel computes a task kernel of\n",
      "     |  specified size :math:`K_{TT}` and returns :math:`K = K_{TT} \\otimes K_{XX}`. as an\n",
      "     |  :obj:`gpytorch.lazy.KroneckerProductLazyTensor`.\n",
      "     |  \n",
      "     |  :param ~gpytorch.kernels.Kernel data_covar_module: Kernel to use as the data kernel.\n",
      "     |  :param int num_tasks: Number of tasks\n",
      "     |  :param int rank: (default 1) Rank of index kernel to use for task covariance matrix.\n",
      "     |  :param ~gpytorch.priors.Prior task_covar_prior: (default None) Prior to use for task kernel.\n",
      "     |      See :class:`gpytorch.kernels.IndexKernel` for details.\n",
      "     |  :param dict kwargs: Additional arguments to pass to the kernel.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultitaskKernel\n",
      "     |      gpytorch.kernels.kernel.Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, data_covar_module: gpytorch.kernels.kernel.Kernel, num_tasks: int, rank: Union[int, NoneType] = 1, task_covar_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, **kwargs)\n",
      "     |  \n",
      "     |  forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Computes the covariance between x1 and x2.\n",
      "     |      This method should be imlemented by all Kernel subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b x n x d`):\n",
      "     |              First set of data\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b x m x d`):\n",
      "     |              Second set of data\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should the Kernel compute the whole kernel, or just the diag?\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              If this is true, it treats the last dimension of the data as another batch dimension.\n",
      "     |              (Useful for additive structure over the dimensions). Default: False\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n",
      "     |              The exact size depends on the kernel's evaluation mode:\n",
      "     |      \n",
      "     |              * `full_covar`: `n x m` or `b x n x m`\n",
      "     |              * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n",
      "     |              * `diag`: `n` or `b x n`\n",
      "     |              * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1, x2)\n",
      "     |      Given `n` data points `x1` and `m` datapoints `x2`, this multitask\n",
      "     |      kernel returns an `(n*num_tasks) x (m*num_tasks)` covariance matrix.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  is_stationary\n",
      "     |      Property to indicate whether kernel is stationary or not.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  has_lengthscale = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class NewtonGirardAdditiveKernel(gpytorch.kernels.kernel.Kernel)\n",
      "     |  NewtonGirardAdditiveKernel(base_kernel: gpytorch.kernels.kernel.Kernel, num_dims: int, max_degree: Union[int, NoneType] = None, active_dims: Union[Tuple[int, ...], NoneType] = None, **kwargs)\n",
      "     |  \n",
      "     |  Kernels in GPyTorch are implemented as a :class:`gpytorch.Module` that, when called on two :obj:`torch.tensor`\n",
      "     |  objects `x1` and `x2` returns either a :obj:`torch.tensor` or a :obj:`gpytorch.lazy.LazyTensor` that represents\n",
      "     |  the covariance matrix between `x1` and `x2`.\n",
      "     |  \n",
      "     |  In the typical use case, to extend this class means to implement the :func:`~gpytorch.kernels.Kernel.forward`\n",
      "     |  method.\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |      The :func:`~gpytorch.kernels.Kernel.__call__` does some additional internal work. In particular,\n",
      "     |      all kernels are lazily evaluated so that, in some cases, we can index in to the kernel matrix before actually\n",
      "     |      computing it. Furthermore, many built in kernel modules return LazyTensors that allow for more efficient\n",
      "     |      inference than if we explicitly computed the kernel matrix itself.\n",
      "     |  \n",
      "     |      As a result, if you want to use a :obj:`gpytorch.kernels.Kernel` object just to get an actual\n",
      "     |      :obj:`torch.tensor` representing the covariance matrix, you may need to call the\n",
      "     |      :func:`gpytorch.lazy.LazyTensor.evaluate` method on the output.\n",
      "     |  \n",
      "     |  This base :class:`Kernel` class includes a lengthscale parameter\n",
      "     |  :math:`\\Theta`, which is used by many common kernel functions.\n",
      "     |  There are a few options for the lengthscale:\n",
      "     |  \n",
      "     |  * Default: No lengthscale (i.e. :math:`\\Theta` is the identity matrix).\n",
      "     |  \n",
      "     |  * Single lengthscale: One lengthscale can be applied to all input dimensions/batches\n",
      "     |    (i.e. :math:`\\Theta` is a constant diagonal matrix).\n",
      "     |    This is controlled by setting the attribute `has_lengthscale=True`.\n",
      "     |  \n",
      "     |  * ARD: Each input dimension gets its own separate lengthscale\n",
      "     |    (i.e. :math:`\\Theta` is a non-constant diagonal matrix).\n",
      "     |    This is controlled by the `ard_num_dims` keyword argument (as well as `has_lengthscale=True`).\n",
      "     |  \n",
      "     |  In batch-mode (i.e. when :math:`x_1` and :math:`x_2` are batches of input matrices), each\n",
      "     |  batch of data can have its own lengthscale parameter by setting the `batch_shape`\n",
      "     |  keyword argument to the appropriate number of batches.\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |  \n",
      "     |      The :attr:`lengthscale` parameter is parameterized on a log scale to constrain it to be positive.\n",
      "     |      You can set a prior on this parameter using the :attr:`lengthscale_prior` argument.\n",
      "     |  \n",
      "     |  Base Args:\n",
      "     |      :attr:`ard_num_dims` (int, optional):\n",
      "     |          Set this if you want a separate lengthscale for each input\n",
      "     |          dimension. It should be `d` if :attr:`x1` is a `n x d` matrix.  Default: `None`\n",
      "     |      :attr:`batch_shape` (torch.Size, optional):\n",
      "     |          Set this if you want a separate lengthscale for each batch of input\n",
      "     |          data. It should be `b1 x ... x bk` if :attr:`x1` is a `b1 x ... x bk x n x d` tensor.\n",
      "     |      :attr:`active_dims` (tuple of ints, optional):\n",
      "     |          Set this if you want to compute the covariance of only a few input dimensions. The ints\n",
      "     |          corresponds to the indices of the dimensions. Default: `None`.\n",
      "     |      :attr:`lengthscale_prior` (Prior, optional):\n",
      "     |          Set this if you want to apply a prior to the lengthscale parameter.  Default: `None`\n",
      "     |      :attr:`lengthscale_constraint` (Constraint, optional):\n",
      "     |          Set this if you want to apply a constraint to the lengthscale parameter. Default: `Positive`.\n",
      "     |      :attr:`eps` (float):\n",
      "     |          The minimum value that the lengthscale can take (prevents divide by zero errors). Default: `1e-6`.\n",
      "     |  \n",
      "     |  Base Attributes:\n",
      "     |      :attr:`lengthscale` (Tensor):\n",
      "     |          The lengthscale parameter. Size/shape of parameter depends on the\n",
      "     |          :attr:`ard_num_dims` and :attr:`batch_shape` arguments.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> covar_module = gpytorch.kernels.LinearKernel()\n",
      "     |      >>> x1 = torch.randn(50, 3)\n",
      "     |      >>> lazy_covar_matrix = covar_module(x1) # Returns a RootLazyTensor\n",
      "     |      >>> tensor_covar_matrix = lazy_covar_matrix.evaluate() # Gets the actual tensor for this kernel matrix\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NewtonGirardAdditiveKernel\n",
      "     |      gpytorch.kernels.kernel.Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, base_kernel: gpytorch.kernels.kernel.Kernel, num_dims: int, max_degree: Union[int, NoneType] = None, active_dims: Union[Tuple[int, ...], NoneType] = None, **kwargs)\n",
      "     |      Create an Additive Kernel a la https://arxiv.org/abs/1112.4394 using Newton-Girard Formulae\n",
      "     |      \n",
      "     |      :param base_kernel: a base 1-dimensional kernel. NOTE: put ard_num_dims=d in the base kernel...\n",
      "     |      :param max_degree: the maximum numbers of kernel degrees to compute\n",
      "     |      :param active_dims:\n",
      "     |      :param kwargs:\n",
      "     |  \n",
      "     |  forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Forward proceeds by Newton-Girard formulae\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  outputscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1, x2)\n",
      "     |      How many outputs are produced per input (default 1)\n",
      "     |      if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel\n",
      "     |      will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`\n",
      "     |      Default: 1\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  is_stationary\n",
      "     |      Property to indicate whether kernel is stationary or not.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  has_lengthscale = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class PeriodicKernel(gpytorch.kernels.kernel.Kernel)\n",
      "     |  PeriodicKernel(period_length_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, period_length_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, **kwargs)\n",
      "     |  \n",
      "     |  Computes a covariance matrix based on the periodic kernel\n",
      "     |  between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`:\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |  \n",
      "     |      \\begin{equation*}\n",
      "     |          k_{\\text{Periodic}}(\\mathbf{x_1}, \\mathbf{x_2}) = \\exp \\left(\n",
      "     |          -2 \\sum_i\n",
      "     |          \\frac{\\sin ^2 \\left( \\frac{\\pi}{p} (\\mathbf{x_{1,i}} - \\mathbf{x_{2,i}} ) \\right)}{\\lambda}\n",
      "     |          \\right)\n",
      "     |      \\end{equation*}\n",
      "     |  \n",
      "     |  where\n",
      "     |  \n",
      "     |  * :math:`p` is the period length parameter.\n",
      "     |  * :math:`\\lambda` is a lengthscale parameter.\n",
      "     |  \n",
      "     |  Equation is based on [David Mackay's Introduction to Gaussian Processes equation 47]\n",
      "     |  (http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.1927&rep=rep1&type=pdf)\n",
      "     |  albeit without feature-specific lengthscales and period lengths. The exponential\n",
      "     |  coefficient was changed and lengthscale is not squared to maintain backwards compatibility\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |  \n",
      "     |      This kernel does not have an `outputscale` parameter. To add a scaling parameter,\n",
      "     |      decorate this kernel with a :class:`gpytorch.kernels.ScaleKernel`.\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |  \n",
      "     |      This kernel does not have an ARD lengthscale or period length option.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      :attr:`batch_shape` (torch.Size, optional):\n",
      "     |          Set this if you want a separate lengthscale for each\n",
      "     |           batch of input data. It should be `b` if :attr:`x1` is a `b x n x d` tensor. Default: `torch.Size([])`.\n",
      "     |      :attr:`active_dims` (tuple of ints, optional):\n",
      "     |          Set this if you want to compute the covariance of only a few input dimensions. The ints\n",
      "     |          corresponds to the indices of the dimensions. Default: `None`.\n",
      "     |      :attr:`period_length_prior` (Prior, optional):\n",
      "     |          Set this if you want to apply a prior to the period length parameter.  Default: `None`.\n",
      "     |      :attr:`lengthscale_prior` (Prior, optional):\n",
      "     |          Set this if you want to apply a prior to the lengthscale parameter.  Default: `None`.\n",
      "     |      :attr:`lengthscale_constraint` (Constraint, optional):\n",
      "     |          Set this if you want to apply a constraint to the value of the lengthscale. Default: `Positive`.\n",
      "     |      :attr:`period_length_constraint` (Constraint, optional):\n",
      "     |          Set this if you want to apply a constraint to the value of the period length. Default: `Positive`.\n",
      "     |      :attr:`eps` (float):\n",
      "     |          The minimum value that the lengthscale/period length can take\n",
      "     |          (prevents divide by zero errors). Default: `1e-6`.\n",
      "     |  \n",
      "     |  Attributes:\n",
      "     |      :attr:`lengthscale` (Tensor):\n",
      "     |          The lengthscale parameter. Size = `*batch_shape x 1 x 1`.\n",
      "     |      :attr:`period_length` (Tensor):\n",
      "     |          The period length parameter. Size = `*batch_shape x 1 x 1`.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> x = torch.randn(10, 5)\n",
      "     |      >>> # Non-batch: Simple option\n",
      "     |      >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.PeriodicKernel())\n",
      "     |      >>>\n",
      "     |      >>> batch_x = torch.randn(2, 10, 5)\n",
      "     |      >>> # Batch: Simple option\n",
      "     |      >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.PeriodicKernel())\n",
      "     |      >>> # Batch: different lengthscale for each batch\n",
      "     |      >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.PeriodicKernel(batch_size=2))\n",
      "     |      >>> covar = covar_module(x)  # Output: LazyVariable of size (2 x 10 x 10)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PeriodicKernel\n",
      "     |      gpytorch.kernels.kernel.Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, period_length_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, period_length_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, x1, x2, diag=False, **params)\n",
      "     |      Computes the covariance between x1 and x2.\n",
      "     |      This method should be imlemented by all Kernel subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b x n x d`):\n",
      "     |              First set of data\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b x m x d`):\n",
      "     |              Second set of data\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should the Kernel compute the whole kernel, or just the diag?\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              If this is true, it treats the last dimension of the data as another batch dimension.\n",
      "     |              (Useful for additive structure over the dimensions). Default: False\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n",
      "     |              The exact size depends on the kernel's evaluation mode:\n",
      "     |      \n",
      "     |              * `full_covar`: `n x m` or `b x n x m`\n",
      "     |              * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n",
      "     |              * `diag`: `n` or `b x n`\n",
      "     |              * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  period_length\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  has_lengthscale = True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1, x2)\n",
      "     |      How many outputs are produced per input (default 1)\n",
      "     |      if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel\n",
      "     |      will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`\n",
      "     |      Default: 1\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  is_stationary\n",
      "     |      Property to indicate whether kernel is stationary or not.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class PiecewisePolynomialKernel(gpytorch.kernels.kernel.Kernel)\n",
      "     |  PiecewisePolynomialKernel(q: Union[int, NoneType] = 2, **kwargs)\n",
      "     |  \n",
      "     |  Computes a covariance matrix based on the Piecewise Polynomial kernel\n",
      "     |  between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`:\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |  \n",
      "     |      \\begin{align}\n",
      "     |          r &= \\left\\Vert x1 - x2 \\right\\Vert \\\\\n",
      "     |          j &= \\lfloor \\frac{D}{2} \\rfloor + q +1 \\\\\n",
      "     |          K_{\\text{ppD, 0}}(\\mathbf{x_1}, \\mathbf{x_2}) &= (1-r)^j_+ , \\\\\n",
      "     |          K_{\\text{ppD, 1}}(\\mathbf{x_1}, \\mathbf{x_2}) &= (1-r)^{j+1}_+ ((j + 1)r + 1), \\\\\n",
      "     |          K_{\\text{ppD, 2}}(\\mathbf{x_1}, \\mathbf{x_2}) &= (1-r)^{j+2}_+ ((1 + (j+2)r +\n",
      "     |              \\frac{j^2 + 4j + 3}{3}r^2), \\\\\n",
      "     |          K_{\\text{ppD, 3}}(\\mathbf{x_1}, \\mathbf{x_2}) &= (1-r)^{j+3}_+\n",
      "     |              (1 + (j+3)r + \\frac{6j^2 + 36j + 45}{15}r^2 +\n",
      "     |              \\frac{j^3 + 9j^2 + 23j +15}{15}r^3) \\\\\n",
      "     |      \\end{align}\n",
      "     |  \n",
      "     |  where :math:`K_{\\text{ppD, q}}` is positive semidefinite in :math:`\\mathbb{R}^{D}` and\n",
      "     |  :math:`q` is the smoothness coefficient. See `Rasmussen and Williams (2006)`_ Equation 4.21.\n",
      "     |  \n",
      "     |  .. note:: This kernel does not have an `outputscale` parameter. To add a scaling parameter,\n",
      "     |      decorate this kernel with a :class:`gpytorch.kernels.ScaleKernel`.\n",
      "     |  \n",
      "     |  :param int q: (default= 2) The smoothness parameter.\n",
      "     |  :type q: int (0, 1, 2 or 3)\n",
      "     |  :param ard_num_dims: (Default: `None`) Set this if you want a separate lengthscale for each\n",
      "     |      input dimension. It should be `d` if :attr:`x1` is a `... x n x d` matrix.\n",
      "     |  :type ard_num_dims: int, optional\n",
      "     |  :param batch_shape: (Default: `None`) Set this if you want a separate lengthscale for each\n",
      "     |       batch of input data. It should be `torch.Size([b1, b2])` for a `b1 x b2 x n x m` kernel output.\n",
      "     |  :type batch_shape: torch.Size, optional\n",
      "     |  :param active_dims: (Default: `None`) Set this if you want to\n",
      "     |      compute the covariance of only a few input dimensions. The ints\n",
      "     |      corresponds to the indices of the dimensions.\n",
      "     |  :type active_dims: Tuple(int)\n",
      "     |  :param lengthscale_prior: (Default: `None`)\n",
      "     |      Set this if you want to apply a prior to the lengthscale parameter.\n",
      "     |  :type lengthscale_prior: ~gpytorch.priors.Prior, optional\n",
      "     |  :param lengthscale_constraint: (Default: `Positive`) Set this if you want\n",
      "     |      to apply a constraint to the lengthscale parameter.\n",
      "     |  :type lengthscale_constraint: ~gpytorch.constraints.Positive, optional\n",
      "     |  :param eps: (Default: 1e-6) The minimum value that the lengthscale can take (prevents divide by zero errors).\n",
      "     |  :type eps: float, optional\n",
      "     |  \n",
      "     |  :var torch.Tensor lengthscale: The lengthscale parameter. Size/shape of parameter depends on the\n",
      "     |      :attr:`ard_num_dims` and :attr:`batch_shape` arguments.\n",
      "     |  \n",
      "     |  .. _Rasmussen and Williams (2006):\n",
      "     |      http://www.gaussianprocess.org/gpml/\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> x = torch.randn(10, 5)\n",
      "     |      >>> # Non-batch option\n",
      "     |      >>> covar_module = gpytorch.kernels.ScaleKernel(\n",
      "     |                              gpytorch.kernels.PiecewisePolynomialKernel(q = 2))\n",
      "     |      >>> # Non-batch: ARD (different lengthscale for each input dimension)\n",
      "     |      >>> covar_module = gpytorch.kernels.ScaleKernel(\n",
      "     |                          gpytorch.kernels.PiecewisePolynomialKernel(q = 2, ard_num_dims=5)\n",
      "     |                          )\n",
      "     |      >>> covar = covar_module(x)  # Output: LazyTensor of size (10 x 10)\n",
      "     |      >>> batch_x = torch.randn(2, 10, 5)\n",
      "     |      >>> # Batch: different lengthscale for each batch\n",
      "     |      >>> covar_module = gpytorch.kernels.ScaleKernel(\n",
      "     |          gpytorch.kernels.PiecewisePolynomialKernel(q = 2, batch_shape=torch.Size([2]))\n",
      "     |          )\n",
      "     |      >>> covar = covar_module(batch_x)  # Output: LazyTensor of size (2 x 10 x 10)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PiecewisePolynomialKernel\n",
      "     |      gpytorch.kernels.kernel.Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, q: Union[int, NoneType] = 2, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  fmax(self, r, j, q)\n",
      "     |  \n",
      "     |  forward(self, x1, x2, last_dim_is_batch=False, diag=False, **params)\n",
      "     |      Computes the covariance between x1 and x2.\n",
      "     |      This method should be imlemented by all Kernel subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b x n x d`):\n",
      "     |              First set of data\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b x m x d`):\n",
      "     |              Second set of data\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should the Kernel compute the whole kernel, or just the diag?\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              If this is true, it treats the last dimension of the data as another batch dimension.\n",
      "     |              (Useful for additive structure over the dimensions). Default: False\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n",
      "     |              The exact size depends on the kernel's evaluation mode:\n",
      "     |      \n",
      "     |              * `full_covar`: `n x m` or `b x n x m`\n",
      "     |              * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n",
      "     |              * `diag`: `n` or `b x n`\n",
      "     |              * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n",
      "     |  \n",
      "     |  get_cov(self, r, j, q)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  has_lengthscale = True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1, x2)\n",
      "     |      How many outputs are produced per input (default 1)\n",
      "     |      if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel\n",
      "     |      will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`\n",
      "     |      Default: 1\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  is_stationary\n",
      "     |      Property to indicate whether kernel is stationary or not.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class PolynomialKernel(gpytorch.kernels.kernel.Kernel)\n",
      "     |  PolynomialKernel(power: int, offset_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, offset_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, **kwargs)\n",
      "     |  \n",
      "     |  Computes a covariance matrix based on the Polynomial kernel\n",
      "     |  between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`:\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |      \\begin{equation*}\n",
      "     |          k_\\text{Poly}(\\mathbf{x_1}, \\mathbf{x_2}) = (\\mathbf{x_1}^\\top\n",
      "     |          \\mathbf{x_2} + c)^{d}.\n",
      "     |      \\end{equation*}\n",
      "     |  \n",
      "     |  where\n",
      "     |  \n",
      "     |  * :math:`c` is an :attr:`offset` parameter.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      :attr:`offset_prior` (:class:`gpytorch.priors.Prior`):\n",
      "     |          Prior over the offset parameter (default `None`).\n",
      "     |      :attr:`offset_constraint` (Constraint, optional):\n",
      "     |          Constraint to place on offset parameter. Default: `Positive`.\n",
      "     |      :attr:`active_dims` (list):\n",
      "     |          List of data dimensions to operate on.\n",
      "     |          `len(active_dims)` should equal `num_dimensions`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PolynomialKernel\n",
      "     |      gpytorch.kernels.kernel.Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, power: int, offset_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, offset_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, x1: torch.Tensor, x2: torch.Tensor, diag: Union[bool, NoneType] = False, last_dim_is_batch: Union[bool, NoneType] = False, **params) -> torch.Tensor\n",
      "     |      Computes the covariance between x1 and x2.\n",
      "     |      This method should be imlemented by all Kernel subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b x n x d`):\n",
      "     |              First set of data\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b x m x d`):\n",
      "     |              Second set of data\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should the Kernel compute the whole kernel, or just the diag?\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              If this is true, it treats the last dimension of the data as another batch dimension.\n",
      "     |              (Useful for additive structure over the dimensions). Default: False\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n",
      "     |              The exact size depends on the kernel's evaluation mode:\n",
      "     |      \n",
      "     |              * `full_covar`: `n x m` or `b x n x m`\n",
      "     |              * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n",
      "     |              * `diag`: `n` or `b x n`\n",
      "     |              * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  offset\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1, x2)\n",
      "     |      How many outputs are produced per input (default 1)\n",
      "     |      if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel\n",
      "     |      will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`\n",
      "     |      Default: 1\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  is_stationary\n",
      "     |      Property to indicate whether kernel is stationary or not.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  has_lengthscale = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class PolynomialKernelGrad(gpytorch.kernels.polynomial_kernel.PolynomialKernel)\n",
      "     |  PolynomialKernelGrad(power: int, offset_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, offset_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, **kwargs)\n",
      "     |  \n",
      "     |  Computes a covariance matrix based on the Polynomial kernel\n",
      "     |  between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`:\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |      \\begin{equation*}\n",
      "     |          k_\\text{Poly}(\\mathbf{x_1}, \\mathbf{x_2}) = (\\mathbf{x_1}^\\top\n",
      "     |          \\mathbf{x_2} + c)^{d}.\n",
      "     |      \\end{equation*}\n",
      "     |  \n",
      "     |  where\n",
      "     |  \n",
      "     |  * :math:`c` is an :attr:`offset` parameter.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      :attr:`offset_prior` (:class:`gpytorch.priors.Prior`):\n",
      "     |          Prior over the offset parameter (default `None`).\n",
      "     |      :attr:`offset_constraint` (Constraint, optional):\n",
      "     |          Constraint to place on offset parameter. Default: `Positive`.\n",
      "     |      :attr:`active_dims` (list):\n",
      "     |          List of data dimensions to operate on.\n",
      "     |          `len(active_dims)` should equal `num_dimensions`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PolynomialKernelGrad\n",
      "     |      gpytorch.kernels.polynomial_kernel.PolynomialKernel\n",
      "     |      gpytorch.kernels.kernel.Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  forward(self, x1: torch.Tensor, x2: torch.Tensor, diag: Union[bool, NoneType] = False, last_dim_is_batch: Union[bool, NoneType] = False, **params) -> torch.Tensor\n",
      "     |      Computes the covariance between x1 and x2.\n",
      "     |      This method should be imlemented by all Kernel subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b x n x d`):\n",
      "     |              First set of data\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b x m x d`):\n",
      "     |              Second set of data\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should the Kernel compute the whole kernel, or just the diag?\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              If this is true, it treats the last dimension of the data as another batch dimension.\n",
      "     |              (Useful for additive structure over the dimensions). Default: False\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n",
      "     |              The exact size depends on the kernel's evaluation mode:\n",
      "     |      \n",
      "     |              * `full_covar`: `n x m` or `b x n x m`\n",
      "     |              * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n",
      "     |              * `diag`: `n` or `b x n`\n",
      "     |              * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1, x2)\n",
      "     |      How many outputs are produced per input (default 1)\n",
      "     |      if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel\n",
      "     |      will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`\n",
      "     |      Default: 1\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.polynomial_kernel.PolynomialKernel:\n",
      "     |  \n",
      "     |  __init__(self, power: int, offset_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, offset_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gpytorch.kernels.polynomial_kernel.PolynomialKernel:\n",
      "     |  \n",
      "     |  offset\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  is_stationary\n",
      "     |      Property to indicate whether kernel is stationary or not.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  has_lengthscale = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class ProductKernel(Kernel)\n",
      "     |  ProductKernel(*kernels)\n",
      "     |  \n",
      "     |  A Kernel that supports elementwise multiplying multiple component kernels together.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> covar_module = RBFKernel(active_dims=torch.tensor([1])) * RBFKernel(active_dims=torch.tensor([2]))\n",
      "     |      >>> x1 = torch.randn(50, 2)\n",
      "     |      >>> kernel_matrix = covar_module(x1) # The RBF Kernel already decomposes multiplicatively, so this is foolish!\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ProductKernel\n",
      "     |      Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __init__(self, *kernels)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, x1, x2, diag=False, **params)\n",
      "     |      Computes the covariance between x1 and x2.\n",
      "     |      This method should be imlemented by all Kernel subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b x n x d`):\n",
      "     |              First set of data\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b x m x d`):\n",
      "     |              Second set of data\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should the Kernel compute the whole kernel, or just the diag?\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              If this is true, it treats the last dimension of the data as another batch dimension.\n",
      "     |              (Useful for additive structure over the dimensions). Default: False\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n",
      "     |              The exact size depends on the kernel's evaluation mode:\n",
      "     |      \n",
      "     |              * `full_covar`: `n x m` or `b x n x m`\n",
      "     |              * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n",
      "     |              * `diag`: `n` or `b x n`\n",
      "     |              * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1, x2)\n",
      "     |      How many outputs are produced per input (default 1)\n",
      "     |      if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel\n",
      "     |      will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`\n",
      "     |      Default: 1\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  is_stationary\n",
      "     |      Kernel is stationary if all components are stationary.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Kernel:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Kernel:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Kernel:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Kernel:\n",
      "     |  \n",
      "     |  has_lengthscale = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class ProductStructureKernel(gpytorch.kernels.kernel.Kernel)\n",
      "     |  ProductStructureKernel(base_kernel: gpytorch.kernels.kernel.Kernel, num_dims: int, active_dims: Union[Tuple[int, ...], NoneType] = None)\n",
      "     |  \n",
      "     |  A Kernel decorator for kernels with product structure. If a kernel decomposes\n",
      "     |  multiplicatively, then this module will be much more computationally efficient.\n",
      "     |  \n",
      "     |  A kernel function `k` has product structure if it can be written as\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |  \n",
      "     |     \\begin{equation*}\n",
      "     |        k(\\mathbf{x_1}, \\mathbf{x_2}) = k'(x_1^{(1)}, x_2^{(1)}) * \\ldots * k'(x_1^{(d)}, x_2^{(d)})\n",
      "     |     \\end{equation*}\n",
      "     |  \n",
      "     |  for some kernel :math:`k'` that operates on each dimension.\n",
      "     |  \n",
      "     |  Given a `b x n x d` input, `ProductStructureKernel` computes `d` one-dimensional kernels\n",
      "     |  (using the supplied base_kernel), and then multiplies the component kernels together.\n",
      "     |  Unlike :class:`~gpytorch.kernels.ProductKernel`, `ProductStructureKernel` computes each\n",
      "     |  of the product terms in batch, making it very fast.\n",
      "     |  \n",
      "     |  See `Product Kernel Interpolation for Scalable Gaussian Processes`_ for more detail.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      - :attr:`base_kernel` (Kernel):\n",
      "     |          The kernel to approximate with KISS-GP\n",
      "     |      - :attr:`num_dims` (int):\n",
      "     |          The dimension of the input data.\n",
      "     |      - :attr:`active_dims` (tuple of ints, optional):\n",
      "     |          Passed down to the `base_kernel`.\n",
      "     |  \n",
      "     |  .. _Product Kernel Interpolation for Scalable Gaussian Processes:\n",
      "     |      https://arxiv.org/pdf/1802.08903\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ProductStructureKernel\n",
      "     |      gpytorch.kernels.kernel.Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, x1_, x2_=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      We cannot lazily evaluate actual kernel calls when using SKIP, because we\n",
      "     |      cannot root decompose rectangular matrices.\n",
      "     |      \n",
      "     |      Because we slice in to the kernel during prediction to get the test x train\n",
      "     |      covar before calling evaluate_kernel, the order of operations would mean we\n",
      "     |      would get a MulLazyTensor representing a rectangular matrix, which we\n",
      "     |      cannot matmul with because we cannot root decompose it. Thus, SKIP actually\n",
      "     |      *requires* that we work with the full (train + test) x (train + test)\n",
      "     |      kernel matrix.\n",
      "     |  \n",
      "     |  __init__(self, base_kernel: gpytorch.kernels.kernel.Kernel, num_dims: int, active_dims: Union[Tuple[int, ...], NoneType] = None)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Computes the covariance between x1 and x2.\n",
      "     |      This method should be imlemented by all Kernel subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b x n x d`):\n",
      "     |              First set of data\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b x m x d`):\n",
      "     |              Second set of data\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should the Kernel compute the whole kernel, or just the diag?\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              If this is true, it treats the last dimension of the data as another batch dimension.\n",
      "     |              (Useful for additive structure over the dimensions). Default: False\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n",
      "     |              The exact size depends on the kernel's evaluation mode:\n",
      "     |      \n",
      "     |              * `full_covar`: `n x m` or `b x n x m`\n",
      "     |              * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n",
      "     |              * `diag`: `n` or `b x n`\n",
      "     |              * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1, x2)\n",
      "     |      How many outputs are produced per input (default 1)\n",
      "     |      if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel\n",
      "     |      will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`\n",
      "     |      Default: 1\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  is_stationary\n",
      "     |      Kernel is stationary if the base kernel is stationary.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  has_lengthscale = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class RBFKernel(gpytorch.kernels.kernel.Kernel)\n",
      "     |  RBFKernel(ard_num_dims: Union[int, NoneType] = None, batch_shape: Union[torch.Size, NoneType] = torch.Size([]), active_dims: Union[Tuple[int, ...], NoneType] = None, lengthscale_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, lengthscale_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, eps: Union[float, NoneType] = 1e-06, **kwargs)\n",
      "     |  \n",
      "     |  Computes a covariance matrix based on the RBF (squared exponential) kernel\n",
      "     |  between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`:\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |  \n",
      "     |     \\begin{equation*}\n",
      "     |        k_{\\text{RBF}}(\\mathbf{x_1}, \\mathbf{x_2}) = \\exp \\left( -\\frac{1}{2}\n",
      "     |        (\\mathbf{x_1} - \\mathbf{x_2})^\\top \\Theta^{-2} (\\mathbf{x_1} - \\mathbf{x_2}) \\right)\n",
      "     |     \\end{equation*}\n",
      "     |  \n",
      "     |  where :math:`\\Theta` is a :attr:`lengthscale` parameter.\n",
      "     |  See :class:`gpytorch.kernels.Kernel` for descriptions of the lengthscale options.\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |  \n",
      "     |      This kernel does not have an `outputscale` parameter. To add a scaling parameter,\n",
      "     |      decorate this kernel with a :class:`gpytorch.kernels.ScaleKernel`.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      :attr:`ard_num_dims` (int, optional):\n",
      "     |          Set this if you want a separate lengthscale for each\n",
      "     |          input dimension. It should be `d` if :attr:`x1` is a `n x d` matrix. Default: `None`\n",
      "     |      :attr:`batch_shape` (torch.Size, optional):\n",
      "     |          Set this if you want a separate lengthscale for each\n",
      "     |          batch of input data. It should be `b` if :attr:`x1` is a `b x n x d` tensor. Default: `torch.Size([])`.\n",
      "     |      :attr:`active_dims` (tuple of ints, optional):\n",
      "     |          Set this if you want to compute the covariance of only a few input dimensions. The ints\n",
      "     |          corresponds to the indices of the dimensions. Default: `None`.\n",
      "     |      :attr:`lengthscale_prior` (Prior, optional):\n",
      "     |          Set this if you want to apply a prior to the lengthscale parameter.  Default: `None`.\n",
      "     |      :attr:`lengthscale_constraint` (Constraint, optional):\n",
      "     |          Set this if you want to apply a constraint to the lengthscale parameter. Default: `Positive`.\n",
      "     |      :attr:`eps` (float):\n",
      "     |          The minimum value that the lengthscale can take (prevents divide by zero errors). Default: `1e-6`.\n",
      "     |  \n",
      "     |  Attributes:\n",
      "     |      :attr:`lengthscale` (Tensor):\n",
      "     |          The lengthscale parameter. Size/shape of parameter depends on the\n",
      "     |          :attr:`ard_num_dims` and :attr:`batch_shape` arguments.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> x = torch.randn(10, 5)\n",
      "     |      >>> # Non-batch: Simple option\n",
      "     |      >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
      "     |      >>> # Non-batch: ARD (different lengthscale for each input dimension)\n",
      "     |      >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=5))\n",
      "     |      >>> covar = covar_module(x)  # Output: LazyTensor of size (10 x 10)\n",
      "     |      >>>\n",
      "     |      >>> batch_x = torch.randn(2, 10, 5)\n",
      "     |      >>> # Batch: Simple option\n",
      "     |      >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
      "     |      >>> # Batch: different lengthscale for each batch\n",
      "     |      >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(batch_shape=torch.Size([2])))\n",
      "     |      >>> covar = covar_module(x)  # Output: LazyTensor of size (2 x 10 x 10)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RBFKernel\n",
      "     |      gpytorch.kernels.kernel.Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  forward(self, x1, x2, diag=False, **params)\n",
      "     |      Computes the covariance between x1 and x2.\n",
      "     |      This method should be imlemented by all Kernel subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b x n x d`):\n",
      "     |              First set of data\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b x m x d`):\n",
      "     |              Second set of data\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should the Kernel compute the whole kernel, or just the diag?\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              If this is true, it treats the last dimension of the data as another batch dimension.\n",
      "     |              (Useful for additive structure over the dimensions). Default: False\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n",
      "     |              The exact size depends on the kernel's evaluation mode:\n",
      "     |      \n",
      "     |              * `full_covar`: `n x m` or `b x n x m`\n",
      "     |              * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n",
      "     |              * `diag`: `n` or `b x n`\n",
      "     |              * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  has_lengthscale = True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __init__(self, ard_num_dims: Union[int, NoneType] = None, batch_shape: Union[torch.Size, NoneType] = torch.Size([]), active_dims: Union[Tuple[int, ...], NoneType] = None, lengthscale_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, lengthscale_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, eps: Union[float, NoneType] = 1e-06, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1, x2)\n",
      "     |      How many outputs are produced per input (default 1)\n",
      "     |      if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel\n",
      "     |      will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`\n",
      "     |      Default: 1\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  is_stationary\n",
      "     |      Property to indicate whether kernel is stationary or not.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class RBFKernelGrad(gpytorch.kernels.rbf_kernel.RBFKernel)\n",
      "     |  RBFKernelGrad(ard_num_dims: Union[int, NoneType] = None, batch_shape: Union[torch.Size, NoneType] = torch.Size([]), active_dims: Union[Tuple[int, ...], NoneType] = None, lengthscale_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, lengthscale_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, eps: Union[float, NoneType] = 1e-06, **kwargs)\n",
      "     |  \n",
      "     |  Computes a covariance matrix of the RBF kernel that models the covariance\n",
      "     |  between the values and partial derivatives for inputs :math:`\\mathbf{x_1}`\n",
      "     |  and :math:`\\mathbf{x_2}`.\n",
      "     |  \n",
      "     |  See :class:`gpytorch.kernels.Kernel` for descriptions of the lengthscale options.\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |  \n",
      "     |      This kernel does not have an `outputscale` parameter. To add a scaling parameter,\n",
      "     |      decorate this kernel with a :class:`gpytorch.kernels.ScaleKernel`.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      :attr:`batch_shape` (torch.Size, optional):\n",
      "     |          Set this if you want a separate lengthscale for each\n",
      "     |           batch of input data. It should be `b` if :attr:`x1` is a `b x n x d` tensor. Default: `torch.Size([])`.\n",
      "     |      :attr:`active_dims` (tuple of ints, optional):\n",
      "     |          Set this if you want to compute the covariance of only a few input dimensions. The ints\n",
      "     |          corresponds to the indices of the dimensions. Default: `None`.\n",
      "     |      :attr:`lengthscale_prior` (Prior, optional):\n",
      "     |          Set this if you want to apply a prior to the lengthscale parameter.  Default: `None`.\n",
      "     |      :attr:`lengthscale_constraint` (Constraint, optional):\n",
      "     |          Set this if you want to apply a constraint to the lengthscale parameter. Default: `Positive`.\n",
      "     |      :attr:`eps` (float):\n",
      "     |          The minimum value that the lengthscale can take (prevents divide by zero errors). Default: `1e-6`.\n",
      "     |  \n",
      "     |  Attributes:\n",
      "     |      :attr:`lengthscale` (Tensor):\n",
      "     |          The lengthscale parameter. Size/shape of parameter depends on the\n",
      "     |          :attr:`ard_num_dims` and :attr:`batch_shape` arguments.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> x = torch.randn(10, 5)\n",
      "     |      >>> # Non-batch: Simple option\n",
      "     |      >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernelGrad())\n",
      "     |      >>> covar = covar_module(x)  # Output: LazyTensor of size (60 x 60), where 60 = n * (d + 1)\n",
      "     |      >>>\n",
      "     |      >>> batch_x = torch.randn(2, 10, 5)\n",
      "     |      >>> # Batch: Simple option\n",
      "     |      >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernelGrad())\n",
      "     |      >>> # Batch: different lengthscale for each batch\n",
      "     |      >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernelGrad(batch_shape=torch.Size([2])))\n",
      "     |      >>> covar = covar_module(x)  # Output: LazyTensor of size (2 x 60 x 60)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RBFKernelGrad\n",
      "     |      gpytorch.kernels.rbf_kernel.RBFKernel\n",
      "     |      gpytorch.kernels.kernel.Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  forward(self, x1, x2, diag=False, **params)\n",
      "     |      Computes the covariance between x1 and x2.\n",
      "     |      This method should be imlemented by all Kernel subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b x n x d`):\n",
      "     |              First set of data\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b x m x d`):\n",
      "     |              Second set of data\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should the Kernel compute the whole kernel, or just the diag?\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              If this is true, it treats the last dimension of the data as another batch dimension.\n",
      "     |              (Useful for additive structure over the dimensions). Default: False\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n",
      "     |              The exact size depends on the kernel's evaluation mode:\n",
      "     |      \n",
      "     |              * `full_covar`: `n x m` or `b x n x m`\n",
      "     |              * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n",
      "     |              * `diag`: `n` or `b x n`\n",
      "     |              * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1, x2)\n",
      "     |      How many outputs are produced per input (default 1)\n",
      "     |      if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel\n",
      "     |      will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`\n",
      "     |      Default: 1\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from gpytorch.kernels.rbf_kernel.RBFKernel:\n",
      "     |  \n",
      "     |  has_lengthscale = True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __init__(self, ard_num_dims: Union[int, NoneType] = None, batch_shape: Union[torch.Size, NoneType] = torch.Size([]), active_dims: Union[Tuple[int, ...], NoneType] = None, lengthscale_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, lengthscale_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, eps: Union[float, NoneType] = 1e-06, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  is_stationary\n",
      "     |      Property to indicate whether kernel is stationary or not.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class RFFKernel(gpytorch.kernels.kernel.Kernel)\n",
      "     |  RFFKernel(num_samples: int, num_dims: Union[int, NoneType] = None, **kwargs)\n",
      "     |  \n",
      "     |  Computes a covariance matrix based on Random Fourier Features with the RBFKernel.\n",
      "     |  \n",
      "     |  Random Fourier features was originally proposed in\n",
      "     |  'Random Features for Large-Scale Kernel Machines' by Rahimi and Recht (2008).\n",
      "     |  Instead of the shifted cosine features from Rahimi and Recht (2008), we use\n",
      "     |  the sine and cosine features which is a lower-variance estimator --- see\n",
      "     |  'On the Error of Random Fourier Features' by Sutherland and Schneider (2015).\n",
      "     |  \n",
      "     |  By Bochner's theorem, any continuous kernel :math:`k` is positive definite\n",
      "     |  if and only if it is the Fourier transform of a non-negative measure :math:`p(\\omega)`, i.e.\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |      \\begin{equation}\n",
      "     |          k(x, x') = k(x - x') = \\int p(\\omega) e^{i(\\omega^\\top (x - x'))} d\\omega.\n",
      "     |      \\end{equation}\n",
      "     |  \n",
      "     |  where :math:`p(\\omega)` is a normalized probability measure if :math:`k(0)=1`.\n",
      "     |  \n",
      "     |  For the RBF kernel,\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |      \\begin{equation}\n",
      "     |      k(\\Delta) = \\exp{(-\\frac{\\Delta^2}{2\\sigma^2})}$ and $p(\\omega) = \\exp{(-\\frac{\\sigma^2\\omega^2}{2})}\n",
      "     |      \\end{equation}\n",
      "     |  \n",
      "     |  where :math:`\\Delta = x - x'`.\n",
      "     |  \n",
      "     |  Given datapoint :math:`x\\in \\mathbb{R}^d`, we can construct its random Fourier features\n",
      "     |  :math:`z(x) \\in \\mathbb{R}^{2D}` by\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |      \\begin{equation}\n",
      "     |      z(x) = \\sqrt{\\frac{1}{D}}\n",
      "     |      \\begin{bmatrix}\n",
      "     |          \\cos(\\omega_1^\\top x)\\\\\n",
      "     |          \\sin(\\omega_1^\\top x)\\\\\n",
      "     |          \\cdots \\\\\n",
      "     |          \\cos(\\omega_D^\\top x)\\\\\n",
      "     |          \\sin(\\omega_D^\\top x)\n",
      "     |      \\end{bmatrix}, \\omega_1, \\ldots, \\omega_D \\sim p(\\omega)\n",
      "     |      \\end{equation}\n",
      "     |  \n",
      "     |  such that we have an unbiased Monte Carlo estimator\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |      \\begin{equation}\n",
      "     |          k(x, x') = k(x - x') \\approx z(x)^\\top z(x') = \\frac{1}{D}\\sum_{i=1}^D \\cos(\\omega_i^\\top (x - x')).\n",
      "     |      \\end{equation}\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |      When this kernel is used in batch mode, the random frequencies are drawn\n",
      "     |      independently across the batch dimension as well by default.\n",
      "     |  \n",
      "     |  :param num_samples: Number of random frequencies to draw. This is :math:`D` in the above\n",
      "     |      papers. This will produce :math:`D` sine features and :math:`D` cosine\n",
      "     |      features for a total of :math:`2D` random Fourier features.\n",
      "     |  :type num_samples: int\n",
      "     |  :param num_dims: (Default `None`.) Dimensionality of the data space.\n",
      "     |      This is :math:`d` in the above papers. Note that if you want an\n",
      "     |      independent lengthscale for each dimension, set `ard_num_dims` equal to\n",
      "     |      `num_dims`. If unspecified, it will be inferred the first time `forward`\n",
      "     |      is called.\n",
      "     |  :type num_dims: int, optional\n",
      "     |  \n",
      "     |  :var torch.Tensor randn_weights: The random frequencies that are drawn once and then fixed.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |  \n",
      "     |      >>> # This will infer `num_dims` automatically\n",
      "     |      >>> kernel= gpytorch.kernels.RFFKernel(num_samples=5)\n",
      "     |      >>> x = torch.randn(10, 3)\n",
      "     |      >>> kxx = kernel(x, x).evaluate()\n",
      "     |      >>> print(kxx.randn_weights.size())\n",
      "     |      torch.Size([3, 5])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RFFKernel\n",
      "     |      gpytorch.kernels.kernel.Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, num_samples: int, num_dims: Union[int, NoneType] = None, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, x1: torch.Tensor, x2: torch.Tensor, diag: bool = False, last_dim_is_batch: bool = False, **kwargs) -> torch.Tensor\n",
      "     |      Computes the covariance between x1 and x2.\n",
      "     |      This method should be imlemented by all Kernel subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b x n x d`):\n",
      "     |              First set of data\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b x m x d`):\n",
      "     |              Second set of data\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should the Kernel compute the whole kernel, or just the diag?\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              If this is true, it treats the last dimension of the data as another batch dimension.\n",
      "     |              (Useful for additive structure over the dimensions). Default: False\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n",
      "     |              The exact size depends on the kernel's evaluation mode:\n",
      "     |      \n",
      "     |              * `full_covar`: `n x m` or `b x n x m`\n",
      "     |              * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n",
      "     |              * `diag`: `n` or `b x n`\n",
      "     |              * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  has_lengthscale = True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1, x2)\n",
      "     |      How many outputs are produced per input (default 1)\n",
      "     |      if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel\n",
      "     |      will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`\n",
      "     |      Default: 1\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  is_stationary\n",
      "     |      Property to indicate whether kernel is stationary or not.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class RQKernel(gpytorch.kernels.kernel.Kernel)\n",
      "     |  RQKernel(alpha_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, **kwargs)\n",
      "     |  \n",
      "     |  Computes a covariance matrix based on the rational quadratic kernel\n",
      "     |  between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`:\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |  \n",
      "     |     \\begin{equation*}\n",
      "     |        k_{\\text{RQ}}(\\mathbf{x_1}, \\mathbf{x_2}) =  \\left(1 + \\frac{1}{2\\alpha}\n",
      "     |        (\\mathbf{x_1} - \\mathbf{x_2})^\\top \\Theta^{-2} (\\mathbf{x_1} - \\mathbf{x_2}) \\right)^{-\\alpha}\n",
      "     |     \\end{equation*}\n",
      "     |  \n",
      "     |  where :math:`\\Theta` is a :attr:`lengthscale` parameter, and :math:`\\alpha` is the\n",
      "     |  rational quadratic relative weighting parameter.\n",
      "     |  See :class:`gpytorch.kernels.Kernel` for descriptions of the lengthscale options.\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |  \n",
      "     |      This kernel does not have an `outputscale` parameter. To add a scaling parameter,\n",
      "     |      decorate this kernel with a :class:`gpytorch.kernels.ScaleKernel`.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      :attr:`ard_num_dims` (int, optional):\n",
      "     |          Set this if you want a separate lengthscale for each\n",
      "     |          input dimension. It should be `d` if :attr:`x1` is a `n x d` matrix. Default: `None`\n",
      "     |      :attr:`batch_shape` (torch.Size, optional):\n",
      "     |          Set this if you want a separate lengthscale for each\n",
      "     |          batch of input data. It should be `b` if :attr:`x1` is a `b x n x d` tensor. Default: `torch.Size([])`.\n",
      "     |      :attr:`active_dims` (tuple of ints, optional):\n",
      "     |          Set this if you want to compute the covariance of only a few input dimensions. The ints\n",
      "     |          corresponds to the indices of the dimensions. Default: `None`.\n",
      "     |      :attr:`lengthscale_prior` (Prior, optional):\n",
      "     |          Set this if you want to apply a prior to the lengthscale parameter.  Default: `None`.\n",
      "     |      :attr:`lengthscale_constraint` (Constraint, optional):\n",
      "     |          Set this if you want to apply a constraint to the lengthscale parameter. Default: `Positive`.\n",
      "     |      :attr:`alpha_constraint` (Constraint, optional):\n",
      "     |          Set this if you want to apply a constraint to the alpha parameter. Default: `Positive`.\n",
      "     |      :attr:`eps` (float):\n",
      "     |          The minimum value that the lengthscale can take (prevents divide by zero errors). Default: `1e-6`.\n",
      "     |  \n",
      "     |  Attributes:\n",
      "     |      :attr:`lengthscale` (Tensor):\n",
      "     |          The lengthscale parameter. Size/shape of parameter depends on the\n",
      "     |          :attr:`ard_num_dims` and :attr:`batch_shape` arguments.\n",
      "     |      :attr:`alpha` (Tensor):\n",
      "     |          The rational quadratic relative weighting parameter. Size/shape of parameter depends\n",
      "     |          on the :attr:`batch_shape` argument\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RQKernel\n",
      "     |      gpytorch.kernels.kernel.Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, x1, x2, diag=False, **params)\n",
      "     |      Computes the covariance between x1 and x2.\n",
      "     |      This method should be imlemented by all Kernel subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b x n x d`):\n",
      "     |              First set of data\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b x m x d`):\n",
      "     |              Second set of data\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should the Kernel compute the whole kernel, or just the diag?\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              If this is true, it treats the last dimension of the data as another batch dimension.\n",
      "     |              (Useful for additive structure over the dimensions). Default: False\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n",
      "     |              The exact size depends on the kernel's evaluation mode:\n",
      "     |      \n",
      "     |              * `full_covar`: `n x m` or `b x n x m`\n",
      "     |              * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n",
      "     |              * `diag`: `n` or `b x n`\n",
      "     |              * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  alpha\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  has_lengthscale = True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1, x2)\n",
      "     |      How many outputs are produced per input (default 1)\n",
      "     |      if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel\n",
      "     |      will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`\n",
      "     |      Default: 1\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  is_stationary\n",
      "     |      Property to indicate whether kernel is stationary or not.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class ScaleKernel(gpytorch.kernels.kernel.Kernel)\n",
      "     |  ScaleKernel(base_kernel: gpytorch.kernels.kernel.Kernel, outputscale_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, outputscale_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, **kwargs)\n",
      "     |  \n",
      "     |  Decorates an existing kernel object with an output scale, i.e.\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |  \n",
      "     |     \\begin{equation*}\n",
      "     |        K_{\\text{scaled}} = \\theta_\\text{scale} K_{\\text{orig}}\n",
      "     |     \\end{equation*}\n",
      "     |  \n",
      "     |  where :math:`\\theta_\\text{scale}` is the `outputscale` parameter.\n",
      "     |  \n",
      "     |  In batch-mode (i.e. when :math:`x_1` and :math:`x_2` are batches of input matrices), each\n",
      "     |  batch of data can have its own `outputscale` parameter by setting the `batch_shape`\n",
      "     |  keyword argument to the appropriate number of batches.\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |      The :attr:`outputscale` parameter is parameterized on a log scale to constrain it to be positive.\n",
      "     |      You can set a prior on this parameter using the :attr:`outputscale_prior` argument.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      :attr:`base_kernel` (Kernel):\n",
      "     |          The base kernel to be scaled.\n",
      "     |      :attr:`batch_shape` (int, optional):\n",
      "     |          Set this if you want a separate outputscale for each batch of input data. It should be `b`\n",
      "     |          if :attr:`x1` is a `b x n x d` tensor. Default: `torch.Size([])`\n",
      "     |      :attr:`outputscale_prior` (Prior, optional): Set this if you want to apply a prior to the outputscale\n",
      "     |          parameter.  Default: `None`\n",
      "     |      :attr:`outputscale_constraint` (Constraint, optional): Set this if you want to apply a constraint to the\n",
      "     |          outputscale parameter. Default: `Positive`.\n",
      "     |  \n",
      "     |  Attributes:\n",
      "     |      :attr:`base_kernel` (Kernel):\n",
      "     |          The kernel module to be scaled.\n",
      "     |      :attr:`outputscale` (Tensor):\n",
      "     |          The outputscale parameter. Size/shape of parameter depends on the :attr:`batch_shape` arguments.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> x = torch.randn(10, 5)\n",
      "     |      >>> base_covar_module = gpytorch.kernels.RBFKernel()\n",
      "     |      >>> scaled_covar_module = gpytorch.kernels.ScaleKernel(base_covar_module)\n",
      "     |      >>> covar = scaled_covar_module(x)  # Output: LazyTensor of size (10 x 10)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ScaleKernel\n",
      "     |      gpytorch.kernels.kernel.Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, base_kernel: gpytorch.kernels.kernel.Kernel, outputscale_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, outputscale_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, x1, x2, last_dim_is_batch=False, diag=False, **params)\n",
      "     |      Computes the covariance between x1 and x2.\n",
      "     |      This method should be imlemented by all Kernel subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b x n x d`):\n",
      "     |              First set of data\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b x m x d`):\n",
      "     |              Second set of data\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should the Kernel compute the whole kernel, or just the diag?\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              If this is true, it treats the last dimension of the data as another batch dimension.\n",
      "     |              (Useful for additive structure over the dimensions). Default: False\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n",
      "     |              The exact size depends on the kernel's evaluation mode:\n",
      "     |      \n",
      "     |              * `full_covar`: `n x m` or `b x n x m`\n",
      "     |              * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n",
      "     |              * `diag`: `n` or `b x n`\n",
      "     |              * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1, x2)\n",
      "     |      How many outputs are produced per input (default 1)\n",
      "     |      if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel\n",
      "     |      will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`\n",
      "     |      Default: 1\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  is_stationary\n",
      "     |      Kernel is stationary if base kernel is stationary.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  outputscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  has_lengthscale = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class SpectralDeltaKernel(gpytorch.kernels.kernel.Kernel)\n",
      "     |  SpectralDeltaKernel(num_dims: int, num_deltas: Union[int, NoneType] = 128, Z_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, batch_shape: Union[torch.Size, NoneType] = torch.Size([]), **kwargs)\n",
      "     |  \n",
      "     |  A kernel that supports spectral learning for GPs, where the underlying spectral density is modeled as a mixture\n",
      "     |  of delta distributions (e.g., with point masses). This has been explored e.g. in Lazaro-Gredilla et al., 2010.\n",
      "     |  \n",
      "     |  Conceptually, this kernel is similar to random Fourier features as implemented in RFFKernel, but instead of sampling\n",
      "     |  a Gaussian to determine the spectrum sites, they are treated as learnable parameters.\n",
      "     |  \n",
      "     |  When using CG for inference, this kernel supports linear space and time (in N) for training and inference.\n",
      "     |  \n",
      "     |  :param int num_dims: Dimensionality of input data that this kernel will operate on. Note that if active_dims is\n",
      "     |      used, this should be the length of the active dim set.\n",
      "     |  :param int num_deltas: Number of point masses to learn.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SpectralDeltaKernel\n",
      "     |      gpytorch.kernels.kernel.Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, num_dims: int, num_deltas: Union[int, NoneType] = 128, Z_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, batch_shape: Union[torch.Size, NoneType] = torch.Size([]), **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, x1, x2, diag=False, **params)\n",
      "     |      Computes the covariance between x1 and x2.\n",
      "     |      This method should be imlemented by all Kernel subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b x n x d`):\n",
      "     |              First set of data\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b x m x d`):\n",
      "     |              Second set of data\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should the Kernel compute the whole kernel, or just the diag?\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              If this is true, it treats the last dimension of the data as another batch dimension.\n",
      "     |              (Useful for additive structure over the dimensions). Default: False\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n",
      "     |              The exact size depends on the kernel's evaluation mode:\n",
      "     |      \n",
      "     |              * `full_covar`: `n x m` or `b x n x m`\n",
      "     |              * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n",
      "     |              * `diag`: `n` or `b x n`\n",
      "     |              * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n",
      "     |  \n",
      "     |  initialize_from_data(self, train_x, train_y)\n",
      "     |      Initialize the point masses for this kernel from the empirical spectrum of the data. To do this, we estimate\n",
      "     |      the empirical spectrum's CDF and then simply sample from it. This is analogous to how the SM kernel's mixture\n",
      "     |      is initialized, but we skip the last step of fitting a GMM to the samples and just use the samples directly.\n",
      "     |  \n",
      "     |  initialize_from_data_simple(self, train_x, train_y, **kwargs)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  Z\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  has_lengthscale = True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1, x2)\n",
      "     |      How many outputs are produced per input (default 1)\n",
      "     |      if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel\n",
      "     |      will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`\n",
      "     |      Default: 1\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  is_stationary\n",
      "     |      Property to indicate whether kernel is stationary or not.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class SpectralMixtureKernel(gpytorch.kernels.kernel.Kernel)\n",
      "     |  SpectralMixtureKernel(num_mixtures: Union[int, NoneType] = None, ard_num_dims: Union[int, NoneType] = 1, batch_shape: Union[torch.Size, NoneType] = torch.Size([]), mixture_scales_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, mixture_scales_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, mixture_means_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, mixture_means_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, mixture_weights_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, mixture_weights_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, **kwargs)\n",
      "     |  \n",
      "     |  Computes a covariance matrix based on the Spectral Mixture Kernel\n",
      "     |  between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`.\n",
      "     |  \n",
      "     |  It was proposed in `Gaussian Process Kernels for Pattern Discovery and Extrapolation`_.\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |      Unlike other kernels,\n",
      "     |  \n",
      "     |          * :attr:`ard_num_dims` **must equal** the number of dimensions of the data.\n",
      "     |          * This kernel should not be combined with a :class:`gpytorch.kernels.ScaleKernel`.\n",
      "     |  \n",
      "     |  :param int num_mixtures: The number of components in the mixture.\n",
      "     |  :param int ard_num_dims: Set this to match the dimensionality of the input.\n",
      "     |      It should be `d` if :attr:`x1` is a `... x n x d` matrix. (Default: `1`.)\n",
      "     |  :param batch_shape: Set this if the data is batch of input data. It should\n",
      "     |      be `b_1 x ... x b_j` if :attr:`x1` is a `b_1 x ... x b_j x n x d` tensor. (Default: `torch.Size([])`.)\n",
      "     |  :type batch_shape: torch.Size, optional\n",
      "     |  :param active_dims: Set this if you want to compute the covariance of only\n",
      "     |      a few input dimensions. The ints corresponds to the indices of the dimensions. (Default: `None`.)\n",
      "     |  :type active_dims: float, optional\n",
      "     |  :param eps: The minimum value that the lengthscale can take (prevents divide by zero errors). (Default: `1e-6`.)\n",
      "     |  :type eps: float, optional\n",
      "     |  \n",
      "     |  :param mixture_scales_prior: A prior to set on the :attr:`mixture_scales` parameter\n",
      "     |  :type mixture_scales_prior: ~gpytorch.priors.Prior, optional\n",
      "     |  :param mixture_scales_constraint: A constraint to set on the :attr:`mixture_scales` parameter\n",
      "     |  :type mixture_scales_constraint: ~gpytorch.constraints.Interval, optional\n",
      "     |  :param mixture_means_prior: A prior to set on the :attr:`mixture_means` parameter\n",
      "     |  :type mixture_means_prior: ~gpytorch.priors.Prior, optional\n",
      "     |  :param mixture_means_constraint: A constraint to set on the :attr:`mixture_means` parameter\n",
      "     |  :type mixture_means_constraint: ~gpytorch.constraints.Interval, optional\n",
      "     |  :param mixture_weights_prior: A prior to set on the :attr:`mixture_weights` parameter\n",
      "     |  :type mixture_weights_prior: ~gpytorch.priors.Prior, optional\n",
      "     |  :param mixture_weights_constraint: A constraint to set on the :attr:`mixture_weights` parameter\n",
      "     |  :type mixture_weights_constraint: ~gpytorch.constraints.Interval, optional\n",
      "     |  \n",
      "     |  :ivar torch.Tensor mixture_scales: The lengthscale parameter. Given\n",
      "     |      `k` mixture components, and `... x n x d` data, this will be of size `... x k x 1 x d`.\n",
      "     |  :ivar torch.Tensor mixture_means: The mixture mean parameters (`... x k x 1 x d`).\n",
      "     |  :ivar torch.Tensor mixture_weights: The mixture weight parameters (`... x k`).\n",
      "     |  \n",
      "     |  Example:\n",
      "     |  \n",
      "     |      >>> # Non-batch\n",
      "     |      >>> x = torch.randn(10, 5)\n",
      "     |      >>> covar_module = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=4, ard_num_dims=5)\n",
      "     |      >>> covar = covar_module(x)  # Output: LazyVariable of size (10 x 10)\n",
      "     |      >>>\n",
      "     |      >>> # Batch\n",
      "     |      >>> batch_x = torch.randn(2, 10, 5)\n",
      "     |      >>> covar_module = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=4, batch_size=2, ard_num_dims=5)\n",
      "     |      >>> covar = covar_module(x)  # Output: LazyVariable of size (10 x 10)\n",
      "     |  \n",
      "     |  .. _Gaussian Process Kernels for Pattern Discovery and Extrapolation:\n",
      "     |      https://arxiv.org/pdf/1302.4245.pdf\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SpectralMixtureKernel\n",
      "     |      gpytorch.kernels.kernel.Kernel\n",
      "     |      gpytorch.module.Module\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, num_mixtures: Union[int, NoneType] = None, ard_num_dims: Union[int, NoneType] = 1, batch_shape: Union[torch.Size, NoneType] = torch.Size([]), mixture_scales_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, mixture_scales_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, mixture_means_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, mixture_means_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, mixture_weights_prior: Union[gpytorch.priors.prior.Prior, NoneType] = None, mixture_weights_constraint: Union[gpytorch.constraints.constraints.Interval, NoneType] = None, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, x1: torch.Tensor, x2: torch.Tensor, diag: bool = False, last_dim_is_batch: bool = False, **params) -> Tuple[torch.Tensor, torch.Tensor]\n",
      "     |      Computes the covariance between x1 and x2.\n",
      "     |      This method should be imlemented by all Kernel subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b x n x d`):\n",
      "     |              First set of data\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b x m x d`):\n",
      "     |              Second set of data\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should the Kernel compute the whole kernel, or just the diag?\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              If this is true, it treats the last dimension of the data as another batch dimension.\n",
      "     |              (Useful for additive structure over the dimensions). Default: False\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n",
      "     |              The exact size depends on the kernel's evaluation mode:\n",
      "     |      \n",
      "     |              * `full_covar`: `n x m` or `b x n x m`\n",
      "     |              * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n",
      "     |              * `diag`: `n` or `b x n`\n",
      "     |              * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n",
      "     |  \n",
      "     |  initialize_from_data(self, train_x: torch.Tensor, train_y: torch.Tensor, **kwargs)\n",
      "     |      Initialize mixture components based on batch statistics of the data. You should use\n",
      "     |      this initialization routine if your observations are not evenly spaced.\n",
      "     |      \n",
      "     |      :param torch.Tensor train_x: Training inputs\n",
      "     |      :param torch.Tensor train_y: Training outputs\n",
      "     |  \n",
      "     |  initialize_from_data_empspect(self, train_x: torch.Tensor, train_y: torch.Tensor)\n",
      "     |      Initialize mixture components based on the empirical spectrum of the data.\n",
      "     |      This will often be better than the standard initialize_from_data method, but it assumes\n",
      "     |      that your inputs are evenly spaced.\n",
      "     |      \n",
      "     |      :param torch.Tensor train_x: Training inputs\n",
      "     |      :param torch.Tensor train_y: Training outputs\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  mixture_means\n",
      "     |  \n",
      "     |  mixture_scales\n",
      "     |  \n",
      "     |  mixture_weights\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  is_stationary = True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __setstate__(self, d)\n",
      "     |  \n",
      "     |  covar_dist(self, x1, x2, diag=False, last_dim_is_batch=False, square_dist=False, dist_postprocess_func=<function default_postprocess_script at 0x7fb371cea9d0>, postprocess=True, **params)\n",
      "     |      This is a helper method for computing the Euclidean distance between\n",
      "     |      all pairs of points in x1 and x2.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
      "     |              First set of data.\n",
      "     |          :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
      "     |              Second set of data.\n",
      "     |          :attr:`diag` (bool):\n",
      "     |              Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n",
      "     |          :attr:`last_dim_is_batch` (tuple, optional):\n",
      "     |              Is the last dimension of the data a batch dimension or not?\n",
      "     |          :attr:`square_dist` (bool):\n",
      "     |              Should we square the distance matrix before returning?\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
      "     |          The shape depends on the kernel's mode\n",
      "     |          * `diag=False`\n",
      "     |          * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
      "     |          * `diag=True`\n",
      "     |          * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
      "     |  \n",
      "     |  local_load_samples(self, samples_dict, memo, prefix)\n",
      "     |      Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n",
      "     |      sampling mechanism.\n",
      "     |      \n",
      "     |      The default behavior here should almost always be called from any overriding class. However, a class may\n",
      "     |      want to add additional functionality, such as reshaping things to account for the fact that parameters will\n",
      "     |      acquire an extra batch dimension corresponding to the number of samples drawn.\n",
      "     |  \n",
      "     |  named_sub_kernels(self)\n",
      "     |  \n",
      "     |  num_outputs_per_input(self, x1, x2)\n",
      "     |      How many outputs are produced per input (default 1)\n",
      "     |      if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel\n",
      "     |      will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`\n",
      "     |      Default: 1\n",
      "     |  \n",
      "     |  prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood)\n",
      "     |  \n",
      "     |  sub_kernels(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |  \n",
      "     |  lengthscale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from gpytorch.kernels.kernel.Kernel:\n",
      "     |  \n",
      "     |  has_lengthscale = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gpytorch.module.Module:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  added_loss_terms(self)\n",
      "     |  \n",
      "     |  constraint_for_parameter_name(self, param_name)\n",
      "     |  \n",
      "     |  constraints(self)\n",
      "     |  \n",
      "     |  hyperparameters(self)\n",
      "     |  \n",
      "     |  initialize(self, **kwargs)\n",
      "     |      Set a value for a parameter\n",
      "     |      \n",
      "     |      kwargs: (param_name, value) - parameter to initialize.\n",
      "     |      Can also initialize recursively by passing in the full name of a\n",
      "     |      parameter. For example if model has attribute model.likelihood,\n",
      "     |      we can initialize the noise with either\n",
      "     |      `model.initialize(**{'likelihood.noise': 0.1})`\n",
      "     |      or\n",
      "     |      `model.likelihood.initialize(noise=0.1)`.\n",
      "     |      The former method would allow users to more easily store the\n",
      "     |      initialization values as one object.\n",
      "     |      \n",
      "     |      Value can take the form of a tensor, a float, or an int\n",
      "     |  \n",
      "     |  load_strict_shapes(self, value)\n",
      "     |  \n",
      "     |  named_added_loss_terms(self)\n",
      "     |      Returns an iterator over module variational strategies, yielding both\n",
      "     |      the name of the variational strategy as well as the strategy itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, VariationalStrategy): Tuple containing the name of the\n",
      "     |              strategy and the strategy\n",
      "     |  \n",
      "     |  named_constraints(self, memo=None, prefix='')\n",
      "     |  \n",
      "     |  named_hyperparameters(self)\n",
      "     |  \n",
      "     |  named_parameters_and_constraints(self)\n",
      "     |  \n",
      "     |  named_priors(self, memo=None, prefix='')\n",
      "     |      Returns an iterator over the module's priors, yielding the name of the prior,\n",
      "     |      the prior, the associated parameter names, and the transformation callable.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n",
      "     |              - the name of the prior\n",
      "     |              - the parent module of the prior\n",
      "     |              - the prior\n",
      "     |              - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n",
      "     |              - the prior's transform to be called on the parameters\n",
      "     |  \n",
      "     |  named_variational_parameters(self)\n",
      "     |  \n",
      "     |  pyro_load_from_samples(self, samples_dict)\n",
      "     |      Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n",
      "     |      is typically produced by a Pyro sampling mechanism.\n",
      "     |      \n",
      "     |      Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n",
      "     |      than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n",
      "     |      the prior to properly set the unconstrained parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n",
      "     |  \n",
      "     |  pyro_sample_from_prior(self)\n",
      "     |      For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n",
      "     |      from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n",
      "     |      \n",
      "     |      This method can be used in a Pyro model to conveniently define pyro sample sites for all\n",
      "     |      parameters of the model that have GPyTorch priors registered to them.\n",
      "     |  \n",
      "     |  register_added_loss_term(self, name)\n",
      "     |  \n",
      "     |  register_constraint(self, param_name, constraint, replace=True)\n",
      "     |  \n",
      "     |  register_parameter(self, name, parameter)\n",
      "     |      Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the parameter\n",
      "     |          :attr:`parameter` (torch.nn.Parameter):\n",
      "     |              The parameter\n",
      "     |  \n",
      "     |  register_prior(self, name, prior, param_or_closure, setting_closure=None)\n",
      "     |      Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          :attr:`name` (str):\n",
      "     |              The name of the prior\n",
      "     |          :attr:`prior` (Prior):\n",
      "     |              The prior to be registered`\n",
      "     |          :attr:`param_or_closure` (string or callable):\n",
      "     |              Either the name of the parameter, or a closure (which upon calling evalutes a function on\n",
      "     |              the module instance and one or more parameters):\n",
      "     |              single parameter without a transform: `.register_prior(\"foo_prior\", foo_prior, \"foo_param\")`\n",
      "     |              transform a single parameter (e.g. put a log-Normal prior on it):\n",
      "     |              `.register_prior(\"foo_prior\", NormalPrior(0, 1), lambda module: torch.log(module.foo_param))`\n",
      "     |              function of multiple parameters:\n",
      "     |              `.register_prior(\"foo2_prior\", foo2_prior, lambda module: f(module.param1, module.param2)))`\n",
      "     |          :attr:`setting_closure` (callable, optional):\n",
      "     |              A function taking in the module instance and a tensor in (transformed) parameter space,\n",
      "     |              initializing the internal parameter representation to the proper value by applying the\n",
      "     |              inverse transform. Enables setting parametres directly in the transformed space, as well\n",
      "     |              as sampling parameter values from priors (see `sample_from_prior`)\n",
      "     |  \n",
      "     |  sample_from_prior(self, prior_name)\n",
      "     |      Sample parameter values from prior. Modifies the module's parameters in-place.\n",
      "     |  \n",
      "     |  to_pyro_random_module(self)\n",
      "     |  \n",
      "     |  train(self, mode=True)\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  update_added_loss_term(self, name, added_loss_term)\n",
      "     |  \n",
      "     |  variational_parameters(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "\n",
      "DATA\n",
      "    __all__ = ['keops', 'Kernel', 'ArcKernel', 'AdditiveKernel', 'Additive...\n",
      "\n",
      "FILE\n",
      "    /Users/erio/opt/anaconda3/envs/QML/lib/python3.8/site-packages/gpytorch/kernels/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gpytorch.kernels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QML",
   "language": "python",
   "name": "qml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
