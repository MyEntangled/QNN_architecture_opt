{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3add0720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.opflow import Z, I\n",
    "from qiskit.opflow import StateFn\n",
    "import numpy as np\n",
    "from qiskit import Aer\n",
    "from qiskit.utils import QuantumInstance\n",
    "from qiskit.circuit import QuantumCircuit\n",
    "from qiskit.opflow import Z, X\n",
    "from qiskit.opflow import CircuitSampler, PauliExpectation\n",
    "from qiskit.opflow import PauliExpectation, CircuitSampler\n",
    "from qiskit.opflow import Gradient\n",
    "from qiskit.algorithms.optimizers import GradientDescent, ADAM\n",
    "from qiskit.circuit.library import RealAmplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72f81378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"word-wrap: normal;white-space: pre;background: #fff0;line-height: 1.1;font-family: &quot;Courier New&quot;,Courier,monospace\">     ┌──────────────────────────────────────┐\n",
       "q_0: ┤0                                     ├\n",
       "     │  RealAmplitudes(θ[0],θ[1],θ[2],θ[3]) │\n",
       "q_1: ┤1                                     ├\n",
       "     └──────────────────────────────────────┘</pre>"
      ],
      "text/plain": [
       "     ┌──────────────────────────────────────┐\n",
       "q_0: ┤0                                     ├\n",
       "     │  RealAmplitudes(θ[0],θ[1],θ[2],θ[3]) │\n",
       "q_1: ┤1                                     ├\n",
       "     └──────────────────────────────────────┘"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial_point = np.random.random(ansatz.num_parameters)\n",
    "initial_point = np.array([0.43253681, 0.09507794, 0.42805949, 0.34210341])\n",
    "hamiltonian = Z ^ Z\n",
    "ansatz = RealAmplitudes(num_qubits=2, reps=1, entanglement='linear')\n",
    "ansatz.draw(style='iqx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f58bea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = Aer.get_backend('qasm_simulator')\n",
    "q_instance = QuantumInstance(backend, shots = 8192, seed_simulator = 2718, seed_transpiler = 2718)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b596d47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8852483900774777"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#expectation = StateFn(hamiltonian, is_measurement=True) @ StateFn(ansatz)\n",
    "#exp_result = expectation.eval()\n",
    "\n",
    "def evaluate_expectation(x):\n",
    "    value_dict = dict(zip(ansatz.parameters, x))\n",
    "    bind_ansatz = ansatz.bind_parameters(value_dict)\n",
    "    \n",
    "    expectation = StateFn(hamiltonian, is_measurement=True) @ StateFn(bind_ansatz)\n",
    "    result = expectation.eval()\n",
    "    \n",
    "    return np.real(result)\n",
    "\n",
    "evaluate_expectation(initial_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "676d5793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.13857597, -0.35720615, -0.24934966, -0.21377437])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_gradient(x):\n",
    "    value_dict = dict(zip(ansatz.parameters, x))\n",
    "    \n",
    "    expectation = StateFn(hamiltonian, is_measurement=True) @ StateFn(ansatz)\n",
    "    gradient = Gradient().convert(expectation).assign_parameters(value_dict)\n",
    "    \n",
    "    result = gradient.eval() \n",
    "    return np.real(result)   \n",
    "\n",
    "evaluate_gradient(initial_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37b6dc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_loss = []\n",
    "def gd_callback(nfevs, x, fx, stepsize):\n",
    "    gd_loss.append(fx)\n",
    "    \n",
    "gd = GradientDescent(maxiter=300, learning_rate=0.01, callback=gd_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32d8ed5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_opt, fx_opt, nfevs = gd.optimize(initial_point.size,    # number of parameters\n",
    "                                   evaluate_expectation,  # function to minimize\n",
    "                                   gradient_function=evaluate_gradient,  # function to evaluate the gradient\n",
    "                                   initial_point=initial_point)  # initial point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d2995da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.31391274  1.47656398  0.33825704  1.60692521] -0.9982793854608492 300\n"
     ]
    }
   ],
   "source": [
    "print(x_opt, fx_opt, nfevs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "960bcd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method optimize in module qiskit.algorithms.optimizers.gradient_descent:\n",
      "\n",
      "optimize(num_vars, objective_function, gradient_function=None, variable_bounds=None, initial_point=None) method of qiskit.algorithms.optimizers.gradient_descent.GradientDescent instance\n",
      "    Perform optimization.\n",
      "    \n",
      "    Args:\n",
      "        num_vars (int) : Number of parameters to be optimized.\n",
      "        objective_function (callable) : A function that\n",
      "            computes the objective function.\n",
      "        gradient_function (callable) : A function that\n",
      "            computes the gradient of the objective function, or\n",
      "            None if not available.\n",
      "        variable_bounds (list[(float, float)]) : List of variable\n",
      "            bounds, given as pairs (lower, upper). None means\n",
      "            unbounded.\n",
      "        initial_point (numpy.ndarray[float]) : Initial point.\n",
      "    \n",
      "    Returns:\n",
      "        point, value, nfev\n",
      "           point: is a 1D numpy.ndarray[float] containing the solution\n",
      "           value: is a float with the objective function value\n",
      "           nfev: number of objective function calls made if available or None\n",
      "    Raises:\n",
      "        ValueError: invalid input\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gd.optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1293691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [1, 2],\n",
       "       [1, 2],\n",
       "       [1, 2],\n",
       "       [1, 2]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([1,2])\n",
    "np.tile(a,(5,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f60f783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 5\n",
    "num_params = 7\n",
    "param_dim = 12\n",
    "input_dim = 2\n",
    "\n",
    "rep_range = np.tile(np.array([num_inputs]), num_params)\n",
    "params = np.random.uniform(0, 2*np.pi, size=(num_params, param_dim))\n",
    "grid_params = np.repeat(params, repeats=rep_range, axis=0)\n",
    "inputs = np.random.normal(0, 1, size=(num_inputs, input_dim))\n",
    "grid_inputs = np.tile(inputs, (num_params, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8b403e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 12)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1f44693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227c211e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9f81488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ADAM in module qiskit.algorithms.optimizers.adam_amsgrad:\n",
      "\n",
      "class ADAM(qiskit.algorithms.optimizers.optimizer.Optimizer)\n",
      " |  ADAM(maxiter: int = 10000, tol: float = 1e-06, lr: float = 0.001, beta_1: float = 0.9, beta_2: float = 0.99, noise_factor: float = 1e-08, eps: float = 1e-10, amsgrad: bool = False, snapshot_dir: Union[str, NoneType] = None) -> None\n",
      " |  \n",
      " |  Adam and AMSGRAD optimizers.\n",
      " |  \n",
      " |  Adam [1] is a gradient-based optimization algorithm that is relies on adaptive estimates of\n",
      " |  lower-order moments. The algorithm requires little memory and is invariant to diagonal\n",
      " |  rescaling of the gradients. Furthermore, it is able to cope with non-stationary objective\n",
      " |  functions and noisy and/or sparse gradients.\n",
      " |  \n",
      " |  AMSGRAD [2] (a variant of Adam) uses a 'long-term memory' of past gradients and, thereby,\n",
      " |  improves convergence properties.\n",
      " |  \n",
      " |  References:\n",
      " |  \n",
      " |      [1]: Kingma, Diederik & Ba, Jimmy (2014), Adam: A Method for Stochastic Optimization.\n",
      " |           `arXiv:1412.6980 <https://arxiv.org/abs/1412.6980>`_\n",
      " |  \n",
      " |      [2]: Sashank J. Reddi and Satyen Kale and Sanjiv Kumar (2018),\n",
      " |           On the Convergence of Adam and Beyond.\n",
      " |           `arXiv:1904.09237 <https://arxiv.org/abs/1904.09237>`_\n",
      " |  \n",
      " |  .. note::\n",
      " |  \n",
      " |      This component has some function that is normally random. If you want to reproduce behavior\n",
      " |      then you should set the random number generator seed in the algorithm_globals\n",
      " |      (``qiskit.utils.algorithm_globals.random_seed = seed``).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ADAM\n",
      " |      qiskit.algorithms.optimizers.optimizer.Optimizer\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, maxiter: int = 10000, tol: float = 1e-06, lr: float = 0.001, beta_1: float = 0.9, beta_2: float = 0.99, noise_factor: float = 1e-08, eps: float = 1e-10, amsgrad: bool = False, snapshot_dir: Union[str, NoneType] = None) -> None\n",
      " |      Args:\n",
      " |          maxiter: Maximum number of iterations\n",
      " |          tol: Tolerance for termination\n",
      " |          lr: Value >= 0, Learning rate.\n",
      " |          beta_1: Value in range 0 to 1, Generally close to 1.\n",
      " |          beta_2: Value in range 0 to 1, Generally close to 1.\n",
      " |          noise_factor: Value >= 0, Noise factor\n",
      " |          eps : Value >=0, Epsilon to be used for finite differences if no analytic\n",
      " |              gradient method is given.\n",
      " |          amsgrad: True to use AMSGRAD, False if not\n",
      " |          snapshot_dir: If not None save the optimizer's parameter\n",
      " |              after every step to the given directory\n",
      " |  \n",
      " |  get_support_level(self)\n",
      " |      Return support level dictionary\n",
      " |  \n",
      " |  load_params(self, load_dir: str) -> None\n",
      " |      Load iteration parameters for a file called ``adam_params.csv``.\n",
      " |      \n",
      " |      Args:\n",
      " |          load_dir: The directory containing ``adam_params.csv``.\n",
      " |  \n",
      " |  minimize(self, objective_function: Callable[[numpy.ndarray], float], initial_point: numpy.ndarray, gradient_function: Callable[[numpy.ndarray], float]) -> Tuple[numpy.ndarray, float, int]\n",
      " |      Run the minimization.\n",
      " |      \n",
      " |      Args:\n",
      " |          objective_function: A function handle to the objective function.\n",
      " |          initial_point: The initial iteration point.\n",
      " |          gradient_function: A function handle to the gradient of the objective function.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tuple of (optimal parameters, optimal value, number of iterations).\n",
      " |  \n",
      " |  optimize(self, num_vars: int, objective_function: Callable[[numpy.ndarray], float], gradient_function: Union[Callable[[numpy.ndarray], float], NoneType] = None, variable_bounds: Union[List[Tuple[float, float]], NoneType] = None, initial_point: Union[numpy.ndarray, NoneType] = None) -> Tuple[numpy.ndarray, float, int]\n",
      " |      Perform optimization.\n",
      " |      \n",
      " |      Args:\n",
      " |          num_vars: Number of parameters to be optimized.\n",
      " |          objective_function: Handle to a function that computes the objective function.\n",
      " |          gradient_function: Handle to a function that computes the gradient of the objective\n",
      " |              function.\n",
      " |          variable_bounds: deprecated\n",
      " |          initial_point: The initial point for the optimization.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tuple (point, value, nfev) where\n",
      " |      \n",
      " |              point: is a 1D numpy.ndarray[float] containing the solution\n",
      " |      \n",
      " |              value: is a float with the objective function value\n",
      " |      \n",
      " |              nfev: is the number of objective function calls\n",
      " |  \n",
      " |  save_params(self, snapshot_dir: str) -> None\n",
      " |      Save the current iteration parameters to a file called ``adam_params.csv``.\n",
      " |      \n",
      " |      Note:\n",
      " |      \n",
      " |          The current parameters are appended to the file, if it exists already.\n",
      " |          The file is not overwritten.\n",
      " |      \n",
      " |      Args:\n",
      " |          snapshot_dir: The directory to store the file in.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  settings\n",
      " |      The optimizer settings in a dictionary format.\n",
      " |      \n",
      " |      The settings can for instance be used for JSON-serialization (if all settings are\n",
      " |      serializable, which e.g. doesn't hold per default for callables), such that the\n",
      " |      optimizer object can be reconstructed as\n",
      " |      \n",
      " |      .. code-block::\n",
      " |      \n",
      " |          settings = optimizer.settings\n",
      " |          # JSON serialize and send to another server\n",
      " |          optimizer = OptimizerClass(**settings)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from qiskit.algorithms.optimizers.optimizer.Optimizer:\n",
      " |  \n",
      " |  print_options(self)\n",
      " |      Print algorithm-specific options.\n",
      " |  \n",
      " |  set_max_evals_grouped(self, limit)\n",
      " |      Set max evals grouped\n",
      " |  \n",
      " |  set_options(self, **kwargs)\n",
      " |      Sets or updates values in the options dictionary.\n",
      " |      \n",
      " |      The options dictionary may be used internally by a given optimizer to\n",
      " |      pass additional optional values for the underlying optimizer/optimization\n",
      " |      function used. The options dictionary may be initially populated with\n",
      " |      a set of key/values when the given optimizer is constructed.\n",
      " |      \n",
      " |      Args:\n",
      " |          kwargs (dict): options, given as name=value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from qiskit.algorithms.optimizers.optimizer.Optimizer:\n",
      " |  \n",
      " |  gradient_num_diff(x_center, f, epsilon, max_evals_grouped=1)\n",
      " |      We compute the gradient with the numeric differentiation in the parallel way,\n",
      " |      around the point x_center.\n",
      " |      \n",
      " |      Args:\n",
      " |          x_center (ndarray): point around which we compute the gradient\n",
      " |          f (func): the function of which the gradient is to be computed.\n",
      " |          epsilon (float): the epsilon used in the numeric differentiation.\n",
      " |          max_evals_grouped (int): max evals grouped\n",
      " |      Returns:\n",
      " |          grad: the gradient computed\n",
      " |  \n",
      " |  wrap_function(function, args)\n",
      " |      Wrap the function to implicitly inject the args at the call of the function.\n",
      " |      \n",
      " |      Args:\n",
      " |          function (func): the target function\n",
      " |          args (tuple): the args to be injected\n",
      " |      Returns:\n",
      " |          function_wrapper: wrapper\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from qiskit.algorithms.optimizers.optimizer.Optimizer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  bounds_support_level\n",
      " |      Returns bounds support level\n",
      " |  \n",
      " |  gradient_support_level\n",
      " |      Returns gradient support level\n",
      " |  \n",
      " |  initial_point_support_level\n",
      " |      Returns initial point support level\n",
      " |  \n",
      " |  is_bounds_ignored\n",
      " |      Returns is bounds ignored\n",
      " |  \n",
      " |  is_bounds_required\n",
      " |      Returns is bounds required\n",
      " |  \n",
      " |  is_bounds_supported\n",
      " |      Returns is bounds supported\n",
      " |  \n",
      " |  is_gradient_ignored\n",
      " |      Returns is gradient ignored\n",
      " |  \n",
      " |  is_gradient_required\n",
      " |      Returns is gradient required\n",
      " |  \n",
      " |  is_gradient_supported\n",
      " |      Returns is gradient supported\n",
      " |  \n",
      " |  is_initial_point_ignored\n",
      " |      Returns is initial point ignored\n",
      " |  \n",
      " |  is_initial_point_required\n",
      " |      Returns is initial point required\n",
      " |  \n",
      " |  is_initial_point_supported\n",
      " |      Returns is initial point supported\n",
      " |  \n",
      " |  setting\n",
      " |      Return setting\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ADAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48f210a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ADAM in module qiskit.algorithms.optimizers.adam_amsgrad:\n",
      "\n",
      "class ADAM(qiskit.algorithms.optimizers.optimizer.Optimizer)\n",
      " |  ADAM(maxiter: int = 10000, tol: float = 1e-06, lr: float = 0.001, beta_1: float = 0.9, beta_2: float = 0.99, noise_factor: float = 1e-08, eps: float = 1e-10, amsgrad: bool = False, snapshot_dir: Union[str, NoneType] = None) -> None\n",
      " |  \n",
      " |  Adam and AMSGRAD optimizers.\n",
      " |  \n",
      " |  Adam [1] is a gradient-based optimization algorithm that is relies on adaptive estimates of\n",
      " |  lower-order moments. The algorithm requires little memory and is invariant to diagonal\n",
      " |  rescaling of the gradients. Furthermore, it is able to cope with non-stationary objective\n",
      " |  functions and noisy and/or sparse gradients.\n",
      " |  \n",
      " |  AMSGRAD [2] (a variant of Adam) uses a 'long-term memory' of past gradients and, thereby,\n",
      " |  improves convergence properties.\n",
      " |  \n",
      " |  References:\n",
      " |  \n",
      " |      [1]: Kingma, Diederik & Ba, Jimmy (2014), Adam: A Method for Stochastic Optimization.\n",
      " |           `arXiv:1412.6980 <https://arxiv.org/abs/1412.6980>`_\n",
      " |  \n",
      " |      [2]: Sashank J. Reddi and Satyen Kale and Sanjiv Kumar (2018),\n",
      " |           On the Convergence of Adam and Beyond.\n",
      " |           `arXiv:1904.09237 <https://arxiv.org/abs/1904.09237>`_\n",
      " |  \n",
      " |  .. note::\n",
      " |  \n",
      " |      This component has some function that is normally random. If you want to reproduce behavior\n",
      " |      then you should set the random number generator seed in the algorithm_globals\n",
      " |      (``qiskit.utils.algorithm_globals.random_seed = seed``).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ADAM\n",
      " |      qiskit.algorithms.optimizers.optimizer.Optimizer\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, maxiter: int = 10000, tol: float = 1e-06, lr: float = 0.001, beta_1: float = 0.9, beta_2: float = 0.99, noise_factor: float = 1e-08, eps: float = 1e-10, amsgrad: bool = False, snapshot_dir: Union[str, NoneType] = None) -> None\n",
      " |      Args:\n",
      " |          maxiter: Maximum number of iterations\n",
      " |          tol: Tolerance for termination\n",
      " |          lr: Value >= 0, Learning rate.\n",
      " |          beta_1: Value in range 0 to 1, Generally close to 1.\n",
      " |          beta_2: Value in range 0 to 1, Generally close to 1.\n",
      " |          noise_factor: Value >= 0, Noise factor\n",
      " |          eps : Value >=0, Epsilon to be used for finite differences if no analytic\n",
      " |              gradient method is given.\n",
      " |          amsgrad: True to use AMSGRAD, False if not\n",
      " |          snapshot_dir: If not None save the optimizer's parameter\n",
      " |              after every step to the given directory\n",
      " |  \n",
      " |  get_support_level(self)\n",
      " |      Return support level dictionary\n",
      " |  \n",
      " |  load_params(self, load_dir: str) -> None\n",
      " |      Load iteration parameters for a file called ``adam_params.csv``.\n",
      " |      \n",
      " |      Args:\n",
      " |          load_dir: The directory containing ``adam_params.csv``.\n",
      " |  \n",
      " |  minimize(self, objective_function: Callable[[numpy.ndarray], float], initial_point: numpy.ndarray, gradient_function: Callable[[numpy.ndarray], float]) -> Tuple[numpy.ndarray, float, int]\n",
      " |      Run the minimization.\n",
      " |      \n",
      " |      Args:\n",
      " |          objective_function: A function handle to the objective function.\n",
      " |          initial_point: The initial iteration point.\n",
      " |          gradient_function: A function handle to the gradient of the objective function.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tuple of (optimal parameters, optimal value, number of iterations).\n",
      " |  \n",
      " |  optimize(self, num_vars: int, objective_function: Callable[[numpy.ndarray], float], gradient_function: Union[Callable[[numpy.ndarray], float], NoneType] = None, variable_bounds: Union[List[Tuple[float, float]], NoneType] = None, initial_point: Union[numpy.ndarray, NoneType] = None) -> Tuple[numpy.ndarray, float, int]\n",
      " |      Perform optimization.\n",
      " |      \n",
      " |      Args:\n",
      " |          num_vars: Number of parameters to be optimized.\n",
      " |          objective_function: Handle to a function that computes the objective function.\n",
      " |          gradient_function: Handle to a function that computes the gradient of the objective\n",
      " |              function.\n",
      " |          variable_bounds: deprecated\n",
      " |          initial_point: The initial point for the optimization.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tuple (point, value, nfev) where\n",
      " |      \n",
      " |              point: is a 1D numpy.ndarray[float] containing the solution\n",
      " |      \n",
      " |              value: is a float with the objective function value\n",
      " |      \n",
      " |              nfev: is the number of objective function calls\n",
      " |  \n",
      " |  save_params(self, snapshot_dir: str) -> None\n",
      " |      Save the current iteration parameters to a file called ``adam_params.csv``.\n",
      " |      \n",
      " |      Note:\n",
      " |      \n",
      " |          The current parameters are appended to the file, if it exists already.\n",
      " |          The file is not overwritten.\n",
      " |      \n",
      " |      Args:\n",
      " |          snapshot_dir: The directory to store the file in.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  settings\n",
      " |      The optimizer settings in a dictionary format.\n",
      " |      \n",
      " |      The settings can for instance be used for JSON-serialization (if all settings are\n",
      " |      serializable, which e.g. doesn't hold per default for callables), such that the\n",
      " |      optimizer object can be reconstructed as\n",
      " |      \n",
      " |      .. code-block::\n",
      " |      \n",
      " |          settings = optimizer.settings\n",
      " |          # JSON serialize and send to another server\n",
      " |          optimizer = OptimizerClass(**settings)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from qiskit.algorithms.optimizers.optimizer.Optimizer:\n",
      " |  \n",
      " |  print_options(self)\n",
      " |      Print algorithm-specific options.\n",
      " |  \n",
      " |  set_max_evals_grouped(self, limit)\n",
      " |      Set max evals grouped\n",
      " |  \n",
      " |  set_options(self, **kwargs)\n",
      " |      Sets or updates values in the options dictionary.\n",
      " |      \n",
      " |      The options dictionary may be used internally by a given optimizer to\n",
      " |      pass additional optional values for the underlying optimizer/optimization\n",
      " |      function used. The options dictionary may be initially populated with\n",
      " |      a set of key/values when the given optimizer is constructed.\n",
      " |      \n",
      " |      Args:\n",
      " |          kwargs (dict): options, given as name=value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from qiskit.algorithms.optimizers.optimizer.Optimizer:\n",
      " |  \n",
      " |  gradient_num_diff(x_center, f, epsilon, max_evals_grouped=1)\n",
      " |      We compute the gradient with the numeric differentiation in the parallel way,\n",
      " |      around the point x_center.\n",
      " |      \n",
      " |      Args:\n",
      " |          x_center (ndarray): point around which we compute the gradient\n",
      " |          f (func): the function of which the gradient is to be computed.\n",
      " |          epsilon (float): the epsilon used in the numeric differentiation.\n",
      " |          max_evals_grouped (int): max evals grouped\n",
      " |      Returns:\n",
      " |          grad: the gradient computed\n",
      " |  \n",
      " |  wrap_function(function, args)\n",
      " |      Wrap the function to implicitly inject the args at the call of the function.\n",
      " |      \n",
      " |      Args:\n",
      " |          function (func): the target function\n",
      " |          args (tuple): the args to be injected\n",
      " |      Returns:\n",
      " |          function_wrapper: wrapper\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from qiskit.algorithms.optimizers.optimizer.Optimizer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  bounds_support_level\n",
      " |      Returns bounds support level\n",
      " |  \n",
      " |  gradient_support_level\n",
      " |      Returns gradient support level\n",
      " |  \n",
      " |  initial_point_support_level\n",
      " |      Returns initial point support level\n",
      " |  \n",
      " |  is_bounds_ignored\n",
      " |      Returns is bounds ignored\n",
      " |  \n",
      " |  is_bounds_required\n",
      " |      Returns is bounds required\n",
      " |  \n",
      " |  is_bounds_supported\n",
      " |      Returns is bounds supported\n",
      " |  \n",
      " |  is_gradient_ignored\n",
      " |      Returns is gradient ignored\n",
      " |  \n",
      " |  is_gradient_required\n",
      " |      Returns is gradient required\n",
      " |  \n",
      " |  is_gradient_supported\n",
      " |      Returns is gradient supported\n",
      " |  \n",
      " |  is_initial_point_ignored\n",
      " |      Returns is initial point ignored\n",
      " |  \n",
      " |  is_initial_point_required\n",
      " |      Returns is initial point required\n",
      " |  \n",
      " |  is_initial_point_supported\n",
      " |      Returns is initial point supported\n",
      " |  \n",
      " |  setting\n",
      " |      Return setting\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ADAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5026906",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = ADAM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df5b4985",
   "metadata": {},
   "outputs": [],
   "source": [
    "f0 = opt.wrap_function(f, y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b3e0bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f0(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b22b782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc0c5fda",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-2c771653a4cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mException\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "raise Exception('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46659eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203ccf57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d1221c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x,y):\n",
    "    return x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ffc4ede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y0 = tuple((1,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QML",
   "language": "python",
   "name": "qml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
